{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65f463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel,BertTokenizerFast\n",
    "import time\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "#src\n",
    "from src.data_utils import load_and_clean_data, prepare_training_pairs3,dataset_preparation, load_samples\n",
    "from src.next_token_dataset import TweetsDataset\n",
    "from src.lstm_model import NextPhrasePredictionRNN\n",
    "from src.eval_lstm import vevaluate3, test_model, analyze_predictions, show_detailed_examples\n",
    "# from src.test_models import test_lstm_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f859fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–∞–±–ª–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: c:\\Users\\OMEN\\Documents\\LLM_Test\\YaPracticum\\project1_text-autocomplete_v3\\data\\test_dataset.csv\n",
      "üîç –ù–∞—á–∞–ª–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...\n",
      "üìä –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: 50000 –ø—Ä–∏–º–µ—Ä–æ–≤\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤:   2%|‚ñè         | 1000/50000 [15:45<12:51:52,  1.06it/s, –ü—Ä–∏–º–µ—Ä=1000/50000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–†–ê–í–ù–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô\n",
      "================================================================================\n",
      "ü§ñ LSTM –º–æ–¥–µ–ª—å:\n",
      "   –°—Ä–µ–¥–Ω–∏–π ROUGE-L: 0.0579\n",
      "   –û–±—â–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: 42.90 —Å–µ–∫\n",
      "   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: 0.0430 —Å–µ–∫\n",
      "ü§ñ GRU –º–æ–¥–µ–ª—å:\n",
      "   –°—Ä–µ–¥–Ω–∏–π ROUGE-L: 0.0614\n",
      "   –û–±—â–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: 16.47 —Å–µ–∫\n",
      "   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: 0.0165 —Å–µ–∫\n",
      "\n",
      "ü§ñ distilGPT2 –º–æ–¥–µ–ª—å:\n",
      "   –°—Ä–µ–¥–Ω–∏–π ROUGE-L: 0.0865\n",
      "   –û–±—â–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: 126.93 —Å–µ–∫\n",
      "   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: 0.1272 —Å–µ–∫\n",
      "\n",
      "üèÜ –†–ï–ô–¢–ò–ù–ì –ú–û–î–ï–õ–ï–ô:\n",
      "   1. distilGPT2: 0.0865\n",
      "   2. GRU: 0.0614 (–æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ: 0.0251)\n",
      "   3. LSTM: 0.0579 (–æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ: 0.0286)\n",
      "\n",
      "üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: c:\\Users\\OMEN\\Documents\\LLM_Test\\YaPracticum\\project1_text-autocomplete_v3\\results\\model_comparison.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_models_lstm_vs_gpt2():\n",
    "    \"\"\"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ LSTM –∏ distilGPT2 –º–æ–¥–µ–ª–µ–π —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–ª–∏–Ω—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\"\"\"\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "    current_dir = Path.cwd()\n",
    "    v_file_path = current_dir / 'data' / 'test_dataset.csv'\n",
    "    test_df = load_samples(v_file_path)\n",
    "    \n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # LSTM –º–æ–¥–µ–ª—å 1\n",
    "    model_path = current_dir / 'models' / 'lstm_50K_best_model.pth'\n",
    "    \n",
    "    lstm_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model_config = checkpoint['model_config']\n",
    "    \n",
    "    lstm_model = NextPhrasePredictionRNN(\n",
    "        rnn_type=\"LSTM\",\n",
    "        vocab_size=model_config['vocab_size'],\n",
    "        emb_dim=model_config['emb_dim'],\n",
    "        hidden_dim=model_config['hidden_dim'],\n",
    "        pad_idx=model_config['pad_idx']\n",
    "    ).to(device)\n",
    "    lstm_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    lstm_model.eval()\n",
    "    \n",
    "    # GRU –º–æ–¥–µ–ª—å 2\n",
    "    model_path2 = current_dir / 'models' / 'best_model.pth'#'gru_MaxL10NumEp100Lim500000NumL2Hidd128Emb300Lr1E-4WDec005_best_model.pth'\n",
    "    # lstm_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    checkpoint2 = torch.load(model_path2, map_location=device)\n",
    "    model_config2 = checkpoint2['model_config']\n",
    "    \n",
    "    lstm_model2 = NextPhrasePredictionRNN(\n",
    "        rnn_type=\"LSTM\",#\"GRU\",\n",
    "        vocab_size=model_config2['vocab_size'],\n",
    "        emb_dim=model_config2['emb_dim'],\n",
    "        hidden_dim=model_config2['hidden_dim'],\n",
    "        pad_idx=model_config2['pad_idx'],\n",
    "        num_layers=model_config2['num_layers']\n",
    "    ).to(device)\n",
    "    lstm_model2.load_state_dict(checkpoint2['model_state_dict'])\n",
    "    lstm_model2.eval()\n",
    "    \n",
    "    \n",
    "    # distilGPT2 –º–æ–¥–µ–ª—å\n",
    "    gpt2_tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "    if gpt2_tokenizer.pad_token is None:\n",
    "        gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "    \n",
    "    gpt2_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device)\n",
    "    gpt2_model.eval()\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ ROUGE –º–µ—Ç—Ä–∏–∫–∏\n",
    "    rouge = evaluate.load('rouge')\n",
    "    \n",
    "    def calculate_expected_tokens_count(original_phrase, truncated_phrase, tokenizer):\n",
    "        \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–∂–∏–¥–∞–µ–º–æ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–∏\"\"\"\n",
    "        full_tokens = tokenizer.tokenize(original_phrase)\n",
    "        truncated_tokens = tokenizer.tokenize(truncated_phrase)\n",
    "        expected_tokens_count = len(full_tokens) - len(truncated_tokens)\n",
    "        return max(expected_tokens_count, 1)  # –º–∏–Ω–∏–º—É–º 1 —Ç–æ–∫–µ–Ω\n",
    "    \n",
    "    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è LSTM —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–ª–∏–Ω—ã\n",
    "    def generate_lstm_completion(model, prompt_text, expected_tokens_count):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        inputs = lstm_tokenizer(prompt_text, return_tensors='pt', truncation=True, max_length=50)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        \n",
    "        current_input = input_ids.clone()\n",
    "        current_mask = attention_mask.clone()\n",
    "        generated_tokens = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(expected_tokens_count):\n",
    "                logits = model(current_input, current_mask)\n",
    "                next_token_logits = logits[0, -1, :]\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "                next_token_id = next_token.item()\n",
    "                \n",
    "                generated_tokens.append(next_token_id)\n",
    "                next_token_tensor = next_token.unsqueeze(0).unsqueeze(0)\n",
    "                current_input = torch.cat([current_input, next_token_tensor], dim=1)\n",
    "                \n",
    "                new_mask = torch.ones(1, 1, device=device, dtype=torch.long)\n",
    "                current_mask = torch.cat([current_mask, new_mask], dim=1)\n",
    "                \n",
    "                # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ EOS —Ç–æ–∫–µ–Ω–µ\n",
    "                if next_token_id == lstm_tokenizer.eos_token_id:\n",
    "                    break\n",
    "        \n",
    "        completion = lstm_tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        end_time = time.time()\n",
    "        generation_time = end_time - start_time\n",
    "        \n",
    "        return completion, generation_time, len(generated_tokens)\n",
    "    \n",
    "    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è GPT2 —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–ª–∏–Ω—ã\n",
    "    def generate_gpt2_completion(prompt_text, expected_tokens_count):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        inputs = gpt2_tokenizer(prompt_text, return_tensors='pt', truncation=True, max_length=50)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=input_ids.shape[1] + expected_tokens_count,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=False,  # greedy decoding\n",
    "                pad_token_id=gpt2_tokenizer.pad_token_id,\n",
    "                eos_token_id=gpt2_tokenizer.eos_token_id,\n",
    "                early_stopping=False,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å\n",
    "        generated_tokens = outputs[0][input_ids.shape[1]:]\n",
    "        \n",
    "        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        if len(generated_tokens) > expected_tokens_count:\n",
    "            generated_tokens = generated_tokens[:expected_tokens_count]\n",
    "        \n",
    "        completion = gpt2_tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        end_time = time.time()\n",
    "        generation_time = end_time - start_time\n",
    "        \n",
    "        return completion, generation_time, len(generated_tokens)\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "    results = []\n",
    "    \n",
    "    print(\"üîç –ù–∞—á–∞–ª–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...\")\n",
    "    print(f\"üìä –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_df)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "    \n",
    "    # for idx, row in test_df.iterrows():\n",
    "    progress_bar = tqdm(test_df.iterrows(), total=len(test_df), desc=\"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "    for idx, row in progress_bar:\n",
    "        if idx>=1000:\n",
    "            break\n",
    "        progress_bar.set_postfix({\n",
    "        \"–ü—Ä–∏–º–µ—Ä\": f\"{idx + 1}/{len(test_df)}\"\n",
    "        # ,\n",
    "        # \"–¢–µ–∫—Å—Ç\": f\"{row['text_cleaned'][:30]}...\" if len(str(row['text_cleaned'])) > 30 else str(row['text_cleaned'])\n",
    "    })\n",
    "        \n",
    "        original_phrase = row['text_cleaned']\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —É—Ä–µ–∑–∞–Ω–Ω—É—é —Ñ—Ä–∞–∑—É (–ø–µ—Ä–≤—ã–µ 5-7 —Å–ª–æ–≤)\n",
    "        # try:\n",
    "        #     words = original_phrase.split()\n",
    "        # except Exception as e:\n",
    "        #     print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ —Ñ—Ä–∞–∑—ã: {e}\")\n",
    "        #     print(f\"–ò—Å—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ original_phrase: '{original_phrase}'\")\n",
    "        \n",
    "        if original_phrase is None or isinstance(original_phrase, float):\n",
    "            continue    \n",
    "        words = original_phrase.split()\n",
    "            \n",
    "        if len(words) > 7:\n",
    "            truncated_phrase = ' '.join(words[:7])\n",
    "        else:\n",
    "            truncated_phrase = ' '.join(words[:-1]) if len(words) > 1 else original_phrase\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–∂–∏–¥–∞–µ–º–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ\n",
    "        expected_continuation = original_phrase[len(truncated_phrase):].strip()\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–∂–∏–¥–∞–µ–º–æ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏\n",
    "        lstm_expected_tokens = calculate_expected_tokens_count(original_phrase, truncated_phrase, lstm_tokenizer)\n",
    "        gpt2_expected_tokens = calculate_expected_tokens_count(original_phrase, truncated_phrase, gpt2_tokenizer)\n",
    "        \n",
    "        # if idx<5:\n",
    "        #     tqdm.write(f\"\\nüìù –ü—Ä–∏–º–µ—Ä {idx + 1}/{len(test_df)}\")\n",
    "        #     tqdm.write(f\"   –û—Ä–∏–≥–∏–Ω–∞–ª: '{original_phrase}'\")\n",
    "        #     tqdm.write(f\"   –£—Ä–µ–∑–∞–Ω–æ: '{truncated_phrase}'\")\n",
    "        #     tqdm.write(f\"   –û–∂–∏–¥–∞–µ–º–æ–µ: '{expected_continuation}'\")\n",
    "        #     tqdm.write(f\"   –û–∂–∏–¥–∞–µ–º–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ - LSTM: {lstm_expected_tokens}, GPT2: {gpt2_expected_tokens}\")\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–π —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–ª–∏–Ω—ã\n",
    "        lstm_completion, lstm_time, lstm_actual_tokens = generate_lstm_completion(lstm_model, truncated_phrase, lstm_expected_tokens)\n",
    "        lstm_completion2, lstm_time2, lstm_actual_tokens2 = generate_lstm_completion(lstm_model2, truncated_phrase, lstm_expected_tokens)\n",
    "        gpt2_completion, gpt2_time, gpt2_actual_tokens = generate_gpt2_completion(truncated_phrase, gpt2_expected_tokens)\n",
    "        \n",
    "        # if idx<5:\n",
    "        #     tqdm.write(f\"   LSTM: '{lstm_completion}' ({lstm_actual_tokens} —Ç–æ–∫–µ–Ω–æ–≤, {lstm_time:.2f}—Å)\")\n",
    "        #     tqdm.write(f\"   GRU: '{lstm_completion2}' ({lstm_actual_tokens2} —Ç–æ–∫–µ–Ω–æ–≤, {lstm_time2:.2f}—Å)\")\n",
    "        #     tqdm.write(f\"   GPT2: '{gpt2_completion}' ({gpt2_actual_tokens} —Ç–æ–∫–µ–Ω–æ–≤, {gpt2_time:.2f}—Å)\")\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º ROUGE –º–µ—Ç—Ä–∏–∫–∏\n",
    "        lstm_rouge = rouge.compute(\n",
    "            predictions=[lstm_completion],\n",
    "            references=[expected_continuation],\n",
    "            use_stemmer=True\n",
    "        ) if lstm_completion.strip() and expected_continuation.strip() else {'rougeL': 0.0}\n",
    "\n",
    "        lstm_rouge2 = rouge.compute(\n",
    "            predictions=[lstm_completion2],\n",
    "            references=[expected_continuation],\n",
    "            use_stemmer=True\n",
    "        ) if lstm_completion2.strip() and expected_continuation.strip() else {'rougeL': 0.0}\n",
    "\n",
    "        \n",
    "        gpt2_rouge = rouge.compute(\n",
    "            predictions=[gpt2_completion],\n",
    "            references=[expected_continuation],\n",
    "            use_stemmer=True\n",
    "        ) if gpt2_completion.strip() and expected_continuation.strip() else {'rougeL': 0.0}\n",
    "        \n",
    "        results.append({\n",
    "            '–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Ñ—Ä–∞–∑–∞': original_phrase,\n",
    "            '–£—Ä–µ–∑–∞–Ω–Ω–∞—è —Ñ—Ä–∞–∑–∞': truncated_phrase,\n",
    "            '–û–∂–∏–¥–∞–µ–º–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ': expected_continuation,\n",
    "            # '–û–∂–∏–¥–∞–µ–º–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ LSTM': lstm_expected_tokens,\n",
    "            # '–û–∂–∏–¥–∞–µ–º–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ GPT2': gpt2_expected_tokens,\n",
    "            '–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã –º–æ–¥–µ–ª–∏ LSTM': lstm_completion,\n",
    "            '–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã –º–æ–¥–µ–ª–∏ GRU': lstm_completion2,\n",
    "            \n",
    "            # '–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ LSTM': lstm_actual_tokens,\n",
    "            '–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã –º–æ–¥–µ–ª–∏ distilGPT2': gpt2_completion,\n",
    "            # '–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ GPT2': gpt2_actual_tokens,\n",
    "            'rouge LSTM': round(lstm_rouge['rougeL'], 4),\n",
    "            'rouge GRU': round(lstm_rouge2['rougeL'], 4),\n",
    "            'rouge distilGPT2': round(gpt2_rouge['rougeL'], 4),\n",
    "            '–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ LSTM': round(lstm_time, 4),\n",
    "            '–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ GRU': round(lstm_time2, 4),\n",
    "            '–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ distilGPT2': round(gpt2_time, 4)\n",
    "        })\n",
    "        \n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV\n",
    "    # output_path = current_dir / 'results' / f'model_comparison_controlled_length_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    output_path = current_dir / 'results' / 'model_comparison.csv'\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    results_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
    "    total_stats = {\n",
    "        '–°—Ä–µ–¥–Ω–∏–π ROUGE-L LSTM': results_df['rouge LSTM'].mean(),\n",
    "        '–°—Ä–µ–¥–Ω–∏–π ROUGE-L GRU': results_df['rouge GRU'].mean(),       \n",
    "        '–°—Ä–µ–¥–Ω–∏–π ROUGE-L GPT2': results_df['rouge distilGPT2'].mean(),\n",
    "        '–û–±—â–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)': results_df['–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ LSTM'].sum(),\n",
    "        '–û–±—â–µ–µ –≤—Ä–µ–º—è GRU (—Å–µ–∫)': results_df['–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ GRU'].sum(),\n",
    "        '–û–±—â–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)': results_df['–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ distilGPT2'].sum(),\n",
    "        '–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)': results_df['–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ LSTM'].mean(),\n",
    "        '–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è GRU (—Å–µ–∫)': results_df['–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ GRU'].mean(),        \n",
    "        '–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)': results_df['–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ distilGPT2'].mean(),\n",
    "        # '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã LSTM (%)': (results_df['–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ LSTM'] == results_df['–û–∂–∏–¥–∞–µ–º–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ LSTM']).mean() * 100,\n",
    "        # '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã GPT2 (%)': (results_df['–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ GPT2'] == results_df['–û–∂–∏–¥–∞–µ–º–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ GPT2']).mean() * 100,\n",
    "        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤': len(results_df)\n",
    "    }\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
    "    # if total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L LSTM'] > total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GPT2']:\n",
    "    #     best_model = \"LSTM\"\n",
    "    #     advantage = total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L LSTM'] - total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GPT2']\n",
    "    # else:\n",
    "    #     best_model = \"distilGPT2\"\n",
    "    #     advantage = total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GPT2'] - total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L LSTM']\n",
    "    \n",
    "    model_scores = {\n",
    "    'LSTM': total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L LSTM'],\n",
    "    'GRU': total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GRU'], \n",
    "    'distilGPT2': total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GPT2']\n",
    "    }\n",
    "    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å    \n",
    "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ —É–±—ã–≤–∞–Ω–∏—é ROUGE-L\n",
    "    sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–†–ê–í–ù–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ü§ñ LSTM –º–æ–¥–µ–ª—å:\")\n",
    "    print(f\"   –°—Ä–µ–¥–Ω–∏–π ROUGE-L: {total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L LSTM']:.4f}\")\n",
    "    # print(f\"   –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã: {total_stats['–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã LSTM (%)']:.1f}%\")\n",
    "    print(f\"   –û–±—â–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)']:.2f} —Å–µ–∫\")\n",
    "    print(f\"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: {total_stats['–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)']:.4f} —Å–µ–∫\")\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    print(f\"ü§ñ GRU –º–æ–¥–µ–ª—å:\")\n",
    "    print(f\"   –°—Ä–µ–¥–Ω–∏–π ROUGE-L: {total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GRU']:.4f}\")\n",
    "    # print(f\"   –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã: {total_stats['–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã LSTM (%)']:.1f}%\")\n",
    "    print(f\"   –û–±—â–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è GRU (—Å–µ–∫)']:.2f} —Å–µ–∫\")\n",
    "    print(f\"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: {total_stats['–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è GRU (—Å–µ–∫)']:.4f} —Å–µ–∫\")\n",
    "    #---------------------------------------------------------------------------------------    \n",
    "    print(f\"\\nü§ñ distilGPT2 –º–æ–¥–µ–ª—å:\")\n",
    "    print(f\"   –°—Ä–µ–¥–Ω–∏–π ROUGE-L: {total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GPT2']:.4f}\")\n",
    "    # print(f\"   –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã: {total_stats['–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã GPT2 (%)']:.1f}%\")\n",
    "    print(f\"   –û–±—â–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)']:.2f} —Å–µ–∫\")\n",
    "    print(f\"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: {total_stats['–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)']:.4f} —Å–µ–∫\")\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # print(f\"\\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model}\")\n",
    "    # print(f\"   –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –≤ ROUGE-L: {advantage:.4f}\")\n",
    "    print(f\"\\nüèÜ –†–ï–ô–¢–ò–ù–ì –ú–û–î–ï–õ–ï–ô:\")\n",
    "    [print(f\"   {i}. {m}: {s:.4f}\" + (f\" (–æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ: {sorted_models[0][1]-s:.4f})\" if i>1 else \"\")) \n",
    "    for i, (m, s) in enumerate(sorted_models, 1)]\n",
    "    \n",
    "    # if total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)'] < total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)']:\n",
    "    #     time_advantage = total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)'] - total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)']\n",
    "    #     print(f\"   LSTM –±—ã—Å—Ç—Ä–µ–µ –Ω–∞: {time_advantage:.2f} —Å–µ–∫\")\n",
    "    # else:\n",
    "    #     time_advantage = total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)'] - total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)']\n",
    "    #     print(f\"   GPT2 –±—ã—Å—Ç—Ä–µ–µ –Ω–∞: {time_advantage:.2f} —Å–µ–∫\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"\\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}\")\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª\n",
    "    stats_df = pd.DataFrame([total_stats])\n",
    "    stats_path = current_dir / 'results' / f'model_stats_controlled_length_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    stats_df.to_csv(stats_path, index=False)\n",
    "    # print(results_df)\n",
    "    return results_df, total_stats#, best_model\n",
    "\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    results, stats = compare_models_lstm_vs_gpt2()\n",
    "    # results, stats, best_model = compare_models_lstm_vs_gpt2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
