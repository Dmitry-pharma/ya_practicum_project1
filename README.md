# ya_practicum_project1
sprint 2 final project

**Цель проекта:**  
сравнить эффективность локально обученной реккурентной сети и модели трансформера по задаче next phrase generation и предложить наиболее эффективный вариант для бизнес-приложения

**Задачи проекта:**  
1) сформировать очищенный датасет на основе сообщений твитера, подготовить выборки (трейн: 80%, валидация: 10%, тест: 10%)  
2) создать и обучить реккурентную нейронную сеть (LSTM) на основе датасета  
3) сравнить эффективность моделей  
4) сформировать выводы и предложения по выбору наилучшей модели (LSTM vs distilgpt2)  

**Структура проекта:**  
```plaintext
ya_practicum_project1/
├── data/                            # Датасеты
│   ├── 1_raw_dataset_tweets.txt     # "сырой" скачанный датасет (не размещен)
│   ├── 2_cleaned_dataset_tweets.csv # "очищенный" датасет, для отладки содержит raw_text и cleaned text (не размещен)
│   └── test_dataset.csv             # сохраненная версия тестового сэмпла
│
├── src/                             # Весь код проекта
│   ├── data_utils.py                # Обработка датасета
|   ├── next_token_dataset.py        # код с torch Dataset'ом 
│   ├── lstm_model.py                # код lstm модели
|   ├── eval_lstm.py                 # замер метрик lstm модели
|   ├── lstm_train.py                # код обучения модели
|   └── visualization.py             # код визуализации - подготовка графиков результатов обучения
│
├── models/                          # веса обученных моделей (не размещены)
|
├── results/                         # итоги сравнения в виде эксель файла
│
├── solution.ipynb                   # ноутбук с обучением и оценкой модели LSTM
├── test_comparison_table2.ipynb     # ноутбук со сравнением моделей LSTM и distilGPT2
└── requirements.txt                 # зависимости проекта 
```
**Описание исходных данных и принципы их очистки**
исходные данные являются твитами (короткими сообщениями) с использованием ников пользователей, хэштэгов, аббревиатур (например "4u" вместо "for you")  
очистка проводилась с использованием:  
• приведение к нижнему регистру    
• преобразование &quot; -> ", &amp; -> &, и т.д. (декодирование HTML тэгов)   
• удалить оставшиеся HTML теги  
• замена упоминаний ников пользователей на USER (@username)    
• удалены хештеги (#tag)    
• удалены эмодзи и специальные символы  
• удалены гиперссылки   
  

**Описание реккурентной модели**

обучение модели возможно в 3х режимах: RNN, LSTM, GRU.   
Результаты приведены для модели LSTM как наилучшие.  
Размерность скрытого слоя = 512
в модели использовались:
- слой нормализации (nn.LayerNorm())
- слой регуляризации (nn.Dropout())
- инициализация весов (xavier_uniform)
  
Результат предсказаний:  
- Всего предсказаний: 1205  
- Правильных: 1136 (94.27%)  
-  Неправильных: 69 (5.73%)  

Результаты обучения LSTM на выборке по основным метрикам:
<img width="1589" height="990" alt="image" src="https://github.com/user-attachments/assets/c9fc87ca-cbcc-439f-98af-5a274ec5bf2c" />

**Описание модели distilgpt**  
модель запускалась на генерацию с параметрами:  
    outputs = model.generate(  
    input_ids,  
    attention_mask=attention_mask,  
      **max_length=input_ids.shape[1] + num_tokens**,  #Ограничиваем длину генерируемого текста  
    **num_return_sequences=1**,  #количество вариантов выводимого текста, выводим только 1 вариант  
    **do_sample=False**, #True (probabilistic sampling - креативность), False (greedy decoding - выбор наиболее вероятного токена)  
    **temperature=0.1**, #детерминированная генерация  
    **no_repeat_ngram_size=2** #убираем зацикливание, 2 → запрещает повторять пары слов, 3 → тройки слов   
        )
        
**Примеры лучших ответов LSTM**  
<img width="2617" height="379" alt="image" src="https://github.com/user-attachments/assets/84a2a598-1a1a-413a-af20-b8422f6bfdc0" />



**Примеры лучших ответов distilGPT2**  
<img width="2617" height="369" alt="image" src="https://github.com/user-attachments/assets/46351b98-e558-4169-b753-472d9a9f4d03" />




**Результаты сравнения**    
```plaintext
    LSTM:  
       Accuracy: 0.3211
       ROUGE-1: 0.4707
  
    Transformer (distilgpt2):
       Accuracy: 0.0092
       ROUGE-1: 0.0136
```

**Выводы:**  
исходя из результатов замеров по метрикам Accuracy, ROUGE и времени генерации преимущество у модели **LSTM**.
качество ответов distilGPT2 выше в плане контекста и смысла, необходимо протестировать возможность дообучения модели distilGPT

**Плюсы и минусы данного проекта и выводов.**
```plaintext
1. сравнение между моделями не совсем справедливое. в виду разных источников для обучения. мы тестируем модель LSTM на гомогенных (похожих) данных с обучением, а модель distilGPT2 обучалась на других данных  
2. ограниченное локальное обучение LSTM. обучение LSTM проводилось локально на ограниченном датасете из 50 000 исходных сообщений (которые потом были токенизированы со смещением 1 токен), к сожалению ВМ не позволила обучить на всей выборке (1.4 млн. сообщений), т.к. проработала около 40 минут и безвозвратно отключилась  
3. при очистке данных не исключалась обсценная лексика 
 
```

