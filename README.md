# ya_practicum_project1
sprint 2 final project

**Цель проекта:**  
сравнить эффективность локально обученной реккурентной сети и модели трансформера по задаче next phrase generation и предложить наиболее эффективный вариант для бизнес-приложения

**Задачи проекта:**  
1) сформировать очищенный датасет на основе сообщений твитера, подготовить выборки (трейн: 80%, валидация: 10%, тест: 10%)  
2) создать и обучить реккурентную нейронную сеть (LSTM) на основе датасета  
3) сравнить эффективность моделей  
4) сформировать выводы и предложения по выбору наилучшей модели (LSTM vs distilgpt2)  

**Структура проекта:**  
```plaintext
ya_practicum_project1/
├── data/                            # Датасеты
│   ├── 1_raw_dataset_tweets.txt     # "сырой" скачанный датасет (не размещен)
│   ├── 2_cleaned_dataset_tweets.csv # "очищенный" датасет, для отладки содержит raw_text и cleaned text (не размещен)
│   └── test_dataset.csv             # сохраненная версия тестового сэмпла
│
├── src/                             # Весь код проекта
│   ├── data_utils.py                # Обработка датасета
|   ├── next_token_dataset.py        # код с torch Dataset'ом 
│   ├── lstm_model.py                # код lstm модели
|   ├── eval_lstm.py                 # замер метрик lstm модели
|   ├── lstm_train.py                # код обучения модели
|   └── visualization.py             # код визуализации - подготовка графиков результатов обучения
│
├── models/                          # веса обученных моделей (не размещены в виду ограничений github)
|
├── results/                         # итоги сравнения в виде эксель файла, можно посмотреть по фразам сравнение вывода трех моделей LSTM,GRU,distilGPT2 с метриками rouge и статистикой времени генерации ответа
│
├── solution.ipynb                   # ноутбук с обучением и оценкой модели LSTM
├── test_comparison_table.ipynb     # ноутбук со сравнением моделей LSTM и distilGPT2
└── requirements.txt                 # зависимости проекта 
```
**Описание исходных данных и принципы их очистки**
исходные данные являются твитами (короткими сообщениями) с использованием ников пользователей, хэштэгов, аббревиатур (например "4u" вместо "for you")  
очистка проводилась с использованием:  
• приведение к нижнему регистру    
• преобразование &quot; -> ", &amp; -> &, и т.д. (декодирование HTML тэгов)   
• удалить оставшиеся HTML теги  
• замена упоминаний ников пользователей на USER (@username)    
• удалены хештеги (#tag)    
• удалены эмодзи и специальные символы  
• удалены гиперссылки   
  

**Описание реккурентной модели**

обучение модели возможно в 3х режимах: RNN, LSTM, GRU.   
Результаты приведены для модели LSTM как наилучшие.  
• Размерность скрытого слоя = 128  
• Количество скрытых слоев =2  
• Количество эпох обучения =100, размер батча =64  
• Скорость обучения = 0,0004  
• weight_decay=0.05  

в модели использовались:
- слой нормализации (nn.LayerNorm())
- слой регуляризации (nn.Dropout())
- инициализация весов (xavier_uniform)
- градиент клиппинг
  
Результаты на тестовой выборке:  
   Test Loss: 4.9178
   Test Accuracy: 0.1692
   Количество примеров: 408430
   
Результаты обучения LSTM на выборке по основным метрикам:
<img width="1595" height="990" alt="image" src="https://github.com/user-attachments/assets/baf7bdad-62c4-48ab-81ab-978c051666ff" />

**Описание модели distilgpt**  
модель запускалась на генерацию с параметрами:  
    outputs = model.generate(  
    input_ids,  
    attention_mask=attention_mask,  
      **max_length=input_ids.shape[1] + num_tokens**,  #Ограничиваем длину генерируемого текста  
    **num_return_sequences=1**,  #количество вариантов выводимого текста, выводим только 1 вариант  
    **do_sample=False**, #True (probabilistic sampling - креативность), False (greedy decoding - выбор наиболее вероятного токена)  
    **temperature=0.1**, #детерминированная генерация, при greedy decoding этот параметр не использовался
    **no_repeat_ngram_size=2** #убираем зацикливание, 2 → запрещает повторять пары слов, 3 → тройки слов   
        )
        
**Примеры лучших ответов LSTM**  
<img width="2617" height="379" alt="image" src="https://github.com/user-attachments/assets/84a2a598-1a1a-413a-af20-b8422f6bfdc0" />



**Примеры лучших ответов distilGPT2**  
<img width="2617" height="369" alt="image" src="https://github.com/user-attachments/assets/46351b98-e558-4169-b753-472d9a9f4d03" />

**Статистика по распределению величин метрики Rouge** - количество полных совпадений (значение 1.0) - выше у distilGPT2, LSTM "угадывает" отдельные слова, поэтому выше частоты в среднем диапазоне метрики rouge (на графике ниже выделены красным шифтом). Количество результатов "полностью мимо" (rouge=0) выше у LSTM.    
<img width="417" height="454" alt="image" src="https://github.com/user-attachments/assets/81356b07-e265-4846-8163-eab62a9d5cdf" />


**Результаты сравнения**    
```plaintext
LSTM модель:
   Средний ROUGE-L: 0.0711
   Среднее время на пример: 0.0084 сек

distilGPT2 модель:
   Средний ROUGE-L: 0.1163
   Среднее время на пример: 0.0414 сек
```

**Выводы:**  
1) исходя из результатов замеров по метрикам ROUGE, преимущество у модели **distilGPT2** (почти в 2 раза), по времени генерации преимущество у модели **LSTM** (почти в 5 раз).
2) выборочный анализ ответов, субъективно говорит о том, что качество ответов distilGPT2 выше в плане контекста и смысла
   
**Плюсы и минусы данного проекта и выводов.**
```plaintext
1. сравнение между моделями не совсем справедливое. в виду разных источников для обучения. мы тестируем модель LSTM на гомогенных (похожих) данных с обучением, а модель distilGPT2 обучалась на других данных. Возможно обсудить дообучение distilGPT2 под данные заказчика.  
2. ограниченное локальное обучение LSTM. обучение LSTM проводилось локально на ограниченном датасете из 50 000 исходных сообщений (которые потом были токенизированы со смещением 1 токен), к сожалению ВМ не позволила обучить на всей выборке (1.4 млн. сообщений), т.к. проработала около 40 минут и безвозвратно отключилась  
3. при очистке данных не исключалась обсценная лексика 
4. согласно ранее опубликованным работам, модели LSTM имеют преимущество при незначительном отставании от трансформеров, возможно использование дополнительных техник для повышения предиктивности за счет увеличения контекста (использовать тему сообщения и предыдущую переписку). Ссылка: https://arxiv.org/pdf/1906.00080
 
```

