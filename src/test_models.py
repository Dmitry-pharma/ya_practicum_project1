import torch
from torch.utils.data import DataLoader
from pathlib import Path
from transformers import AutoTokenizer, GPT2LMHeadModel,BertTokenizerFast
import random
from sklearn.model_selection import train_test_split
import evaluate
from sklearn.metrics import accuracy_score
#src
from src.data_utils import load_and_clean_data, prepare_training_pairs,dataset_preparation
from src.next_token_dataset import TweetsDataset
from src.lstm_model import NextPhrasePredictionRNN
from src.eval_lstm import vevaluate, test_model, analyze_predictions, show_detailed_examples

def create_test_dataset(file_path, limit=1000, max_len=20):
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É –Ω–∞ 1000 –∑–∞–ø–∏—Å–µ–π"""
    print("üìä –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    texts_df = load_and_clean_data(file_path, limit)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º BertTokenizer –¥–ª—è LSTM –º–æ–¥–µ–ª–∏
    lstm_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM
    data = prepare_training_pairs(texts_df, lstm_tokenizer, max_len)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–Ω–µ —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test)
    test_data = data
    
    X_test, Y_test, M_test = zip(*test_data)
    
    # –°–æ–∑–¥–∞–µ–º dataset –∏ loader –¥–ª—è LSTM
    test_ds = TweetsDataset(X_test, Y_test, M_test)
    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)
    
    print(f"‚úÖ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∞: {len(test_ds)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    return test_loader, lstm_tokenizer, texts_df

def test_lstm_model(model_path, test_loader, tokenizer, device):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç LSTM –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ"""
    print("\n" + "="*60)
    print("üß† –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï LSTM –ú–û–î–ï–õ–ò")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
    checkpoint = torch.load(model_path, map_location=device)
    model_config = checkpoint['model_config']
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = NextPhrasePredictionRNN(
        rnn_type="LSTM",
        vocab_size=model_config['vocab_size'],
        emb_dim=model_config['emb_dim'],
        hidden_dim=model_config['hidden_dim'],
        pad_idx=model_config['pad_idx'],
        num_layers=model_config['num_layers']
    ).to(device)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    criterion = torch.nn.CrossEntropyLoss(ignore_index=model_config['pad_idx'])
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    test_accuracy, test_loss = test_model(model, test_loader, criterion, device)
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å ROUGE –º–µ—Ç—Ä–∏–∫–∞–º–∏
    accuracy, avg_loss, rouge_metrics = vevaluate(
        model, test_loader, criterion, device, tokenizer, 
        compute_rouge=True, num_rouge_examples=100
    )
    
    print(f"\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã LSTM:")
    print(f"   Test Accuracy: {accuracy:.4f}")
    print(f"   Test Loss: {avg_loss:.4f}")
    if rouge_metrics and 'rouge1' in rouge_metrics:
        print(f"   ROUGE-1: {rouge_metrics['rouge1']:.4f}")
        print(f"   ROUGE-2: {rouge_metrics['rouge2']:.4f}")
        print(f"   ROUGE-L: {rouge_metrics['rougeL']:.4f}")
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    bad_cases, good_cases = analyze_predictions(model, test_loader, tokenizer, device)
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    show_detailed_examples(model, test_loader, tokenizer, num_examples=3)
    
    return {
        'accuracy': accuracy,
        'loss': avg_loss,
        'rouge_metrics': rouge_metrics,
        'model_type': 'LSTM'
    }

def prepare_transformer_dataset_old(texts_df, tokenizer, max_length=20, num_examples=100):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ next-token prediction"""
    print("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞...")
    
    data = []
    for text in texts_df['text_cleaned'][:num_examples]:
        if len(text.strip()) < 5:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
            continue
            
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        inputs = tokenizer(text, truncation=True, max_length=max_length, return_tensors="pt")
        input_ids = inputs['input_ids'][0]
        
        if len(input_ids) < 2:  # –ù—É–∂–µ–Ω –∫–∞–∫ –º–∏–Ω–∏–º—É–º 1 —Ç–æ–∫–µ–Ω –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ 1 –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            continue
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã next-token prediction
        for i in range(len(input_ids) - 1):
            context = input_ids[:i+1]
            target = input_ids[i+1]
            
            # –ü–∞–¥–¥–∏–Ω–≥ –¥–æ max_length
            if len(context) < max_length:
                padding = torch.full((max_length - len(context),), tokenizer.pad_token_id)
                context = torch.cat([padding, context])
            else:
                context = context[-max_length:]
            
            attention_mask = (context != tokenizer.pad_token_id).long()
            
            data.append({
                'input_ids': context,
                'attention_mask': attention_mask,
                'labels': target.unsqueeze(0)  # –î–æ–±–∞–≤–ª—è–µ–º dimension –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            })
    
    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞")
    return data

def prepare_transformer_dataset(test_ds, tokenizer, max_length=20, num_examples=100):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ next-token prediction –∏–∑ TweetsDataset"""
    print("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏–∑ test_ds...")
    
    data = []
    examples_processed = 0
    
    for i in range(min(len(test_ds), num_examples)):
        if examples_processed >= num_examples:
            break
            
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä –∏–∑ test_ds
        batch = test_ds[i]
        input_ids = batch['data']  # [max_length]
        targets = batch['target']  # [max_length]
        attention_mask = batch['mask']  # [max_length]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–∞–¥–¥–∏–Ω–≥)
        non_pad_indices = (attention_mask == 1).nonzero(as_tuple=True)[0]
        
        if len(non_pad_indices) < 2:  # –ù—É–∂–Ω–æ –∫–∞–∫ –º–∏–Ω–∏–º—É–º 2 —Ç–æ–∫–µ–Ω–∞
            continue
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        real_input_ids = input_ids[non_pad_indices].tolist()
        real_targets = targets[non_pad_indices].tolist()
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã next-token prediction
        for j in range(len(real_input_ids) - 1):
            if examples_processed >= num_examples:
                break
                
            context = real_input_ids[:j+1]  # –¢–æ–∫–µ–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            target = real_targets[j]        # –°–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω
            
            # –ü–∞–¥–¥–∏–Ω–≥ —Å–ª–µ–≤–∞ –¥–æ max_length
            if len(context) < max_length:
                padding = [tokenizer.pad_token_id] * (max_length - len(context))
                padded_context = padding + context
            else:
                padded_context = context[-max_length:]
            
            # –°–æ–∑–¥–∞–µ–º attention_mask
            padded_attention_mask = [1 if token_id != tokenizer.pad_token_id else 0 
                                   for token_id in padded_context]
            
            data.append({
                'input_ids': torch.tensor(padded_context, dtype=torch.long),
                'attention_mask': torch.tensor(padded_attention_mask, dtype=torch.long),
                'labels': torch.tensor([target], dtype=torch.long)  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å transformers
            })
            
            examples_processed += 1
    
    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏–∑ test_ds")
    return data

def prepare_transformer_dataset_from_loader(test_loader, tokenizer, max_length=20, num_examples=100):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏–∑ DataLoader"""
    print("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏–∑ test_loader...")
    
    data = []
    examples_processed = 0
    
    for batch in test_loader:
        if examples_processed >= num_examples:
            break
            
        input_ids_batch = batch['data']      # [batch_size, max_length]
        targets_batch = batch['target']      # [batch_size, max_length]
        attention_mask_batch = batch['mask'] # [batch_size, max_length]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –≤ –±–∞—Ç—á–µ
        for i in range(input_ids_batch.size(0)):
            if examples_processed >= num_examples:
                break
                
            input_ids = input_ids_batch[i]    # [max_length]
            targets = targets_batch[i]        # [max_length]
            attention_mask = attention_mask_batch[i]  # [max_length]
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–∞–¥–¥–∏–Ω–≥)
            non_pad_indices = (attention_mask == 1).nonzero(as_tuple=True)[0]
            
            if len(non_pad_indices) < 2:
                continue
            
            real_input_ids = input_ids[non_pad_indices].tolist()
            real_targets = targets[non_pad_indices].tolist()
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã next-token prediction
            for j in range(len(real_input_ids) - 1):
                if examples_processed >= num_examples:
                    break
                    
                context = real_input_ids[:j+1]
                target = real_targets[j]
                
                # –ü–∞–¥–¥–∏–Ω–≥ —Å–ª–µ–≤–∞
                if len(context) < max_length:
                    padding = [tokenizer.pad_token_id] * (max_length - len(context))
                    padded_context = padding + context
                else:
                    padded_context = context[-max_length:]
                
                padded_attention_mask = [1 if token_id != tokenizer.pad_token_id else 0 
                                       for token_id in padded_context]
                
                data.append({
                    'input_ids': torch.tensor(padded_context, dtype=torch.long),
                    'attention_mask': torch.tensor(padded_attention_mask, dtype=torch.long),
                    'labels': torch.tensor([target], dtype=torch.long)
                })
                
                examples_processed += 1
    
    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞")
    return data



def test_transformer_model_old(model_name, texts_df, device, max_length=20, num_examples=100):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –º–æ–¥–µ–ª—å (DistilGPT2) –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ"""
    print("\n" + "="*60)
    print("ü§ñ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï TRANSFORMER –ú–û–î–ï–õ–ò (DistilGPT2)")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)
    
    # –î–æ–±–∞–≤–ª—è–µ–º pad token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model.eval()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    transformer_data = prepare_transformer_dataset(texts_df, tokenizer, max_length, num_examples)
    
    if not transformer_data:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞")
        return None
    
    total_correct = 0
    total_tokens = 0
    all_predictions = []
    all_references = []
    
    with torch.no_grad():
        for i, example in enumerate(transformer_data):
            input_ids = example['input_ids'].unsqueeze(0).to(device)  # Add batch dimension
            attention_mask = example['attention_mask'].unsqueeze(0).to(device)
            target = example['labels'].to(device)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            
            # –ë–µ—Ä–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
            next_token_logits = logits[:, -1, :]
            predicted_token = torch.argmax(next_token_logits, dim=-1)
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —Ü–µ–ª–µ–≤—ã–º —Ç–æ–∫–µ–Ω–æ–º
            if predicted_token.item() == target.item():
                total_correct += 1
            total_tokens += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            try:
                true_token_text = tokenizer.decode(target.cpu(), skip_special_tokens=True)
                pred_token_text = tokenizer.decode(predicted_token.cpu(), skip_special_tokens=True)
                
                if true_token_text.strip() and pred_token_text.strip():
                    all_references.append(true_token_text)
                    all_predictions.append(pred_token_text)
            except Exception as e:
                continue
            
            if (i + 1) % 50 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(transformer_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –í—ã—á–∏—Å–ª—è–µ–º accuracy
    accuracy = total_correct / total_tokens if total_tokens > 0 else 0
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Transformer ({model_name}):")
    print(f"   Test Accuracy: {accuracy:.4f}")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}")
    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {total_correct}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º ROUGE –º–µ—Ç—Ä–∏–∫–∏
    rouge_metrics = None
    if all_predictions and all_references:
        try:
            
            rouge = evaluate.load('rouge')
            rouge_results = rouge.compute(
                predictions=all_predictions,
                references=all_references,
                use_stemmer=True,
                use_aggregator=True
            )
            
            rouge_metrics = {
                'rouge1': round(rouge_results['rouge1'], 4),
                'rouge2': round(rouge_results['rouge2'], 4),
                'rougeL': round(rouge_results['rougeL'], 4),
                'rougeLsum': round(rouge_results['rougeLsum'], 4),
                'num_examples': len(all_references)
            }
            
            print(f"   ROUGE-1: {rouge_metrics['rouge1']:.4f}")
            print(f"   ROUGE-2: {rouge_metrics['rouge2']:.4f}")
            print(f"   ROUGE-L: {rouge_metrics['rougeL']:.4f}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ ROUGE: {e}")
            rouge_metrics = None
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ROUGE –º–µ—Ç—Ä–∏–∫")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π Transformer:")
    examples_to_show = min(5, len(all_predictions))
    for i in range(examples_to_show):
        print(f"   –ü—Ä–∏–º–µ—Ä {i+1}:")
        print(f"      –û–∂–∏–¥–∞–ª–æ—Å—å: '{all_references[i]}'")
        print(f"      –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: '{all_predictions[i]}'")
        status = "‚úÖ" if all_references[i] == all_predictions[i] else "‚ùå"
        print(f"      –°—Ç–∞—Ç—É—Å: {status}")
        print()
    
    return {
        'accuracy': accuracy,
        'rouge_metrics': rouge_metrics,
        'model_type': 'Transformer',
        'model_name': model_name
    }


def test_transformer_with_generation_old(model_name, test_loader, device, max_length=20, num_examples=50):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
    print("\n" + "="*60)
    print("ü§ñ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï TRANSFORMER –ú–û–î–ï–õ–ò (–†–ï–ñ–ò–ú –ì–ï–ù–ï–†–ê–¶–ò–ò)")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model.eval()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    transformer_data = prepare_transformer_dataset_from_loader(
        test_loader, tokenizer, max_length=max_length, num_examples=num_examples
    )
    
    if not transformer_data:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞")
        return None
    
    all_predictions = []
    all_references = []
    all_contexts = []
    
    print(f"üß™ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ {len(transformer_data)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
    
    with torch.no_grad():
        for i, example in enumerate(transformer_data):
            input_ids = example['input_ids'].unsqueeze(0).to(device)
            attention_mask = example['attention_mask'].unsqueeze(0).to(device)
            target = example['labels'].to(device)
            
            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ
                generated = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=input_ids.shape[1] + 1,  # +1 —Ç–æ–∫–µ–Ω
                    num_return_sequences=1,
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                context_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
                generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
                continuation = generated_text[len(context_text):].strip()
                
                true_token_text = tokenizer.decode(target.cpu(), skip_special_tokens=True)
                
                if true_token_text.strip() and continuation.strip():
                    all_contexts.append(context_text)
                    all_references.append(true_token_text)
                    all_predictions.append(continuation)
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä
                    if i == 0:
                        print(f"\nüîç –ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
                        print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç: '{context_text}'")
                        print(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: '{true_token_text}'")
                        print(f"   –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{continuation}'")
                        
            except Exception as e:
                continue
            
            if (i + 1) % 10 == 0:
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(transformer_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –í—ã—á–∏—Å–ª—è–µ–º ROUGE –º–µ—Ç—Ä–∏–∫–∏
    rouge_metrics = None
    if all_predictions and all_references:
        try:
            
            rouge = evaluate.load('rouge')
            rouge_results = rouge.compute(
                predictions=all_predictions,
                references=all_references,
                use_stemmer=True,
                use_aggregator=True
            )
            
            rouge_metrics = {
                'rouge1': round(rouge_results['rouge1'], 4),
                'rouge2': round(rouge_results['rouge2'], 4),
                'rougeL': round(rouge_results['rougeL'], 4),
                'num_examples': len(all_references)
            }
            
            print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ({model_name}):")
            print(f"   ROUGE-1: {rouge_metrics['rouge1']:.4f}")
            print(f"   ROUGE-2: {rouge_metrics['rouge2']:.4f}")
            print(f"   ROUGE-L: {rouge_metrics['rougeL']:.4f}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ ROUGE: {e}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
    examples_to_show = min(3, len(all_predictions))
    for i in range(examples_to_show):
        print(f"   –ü—Ä–∏–º–µ—Ä {i+1}:")
        print(f"      –ö–æ–Ω—Ç–µ–∫—Å—Ç: '{all_contexts[i]}'")
        print(f"      –û–∂–∏–¥–∞–ª–æ—Å—å: '{all_references[i]}'")
        print(f"      –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{all_predictions[i]}'")
        print()
    
    return {
        'rouge_metrics': rouge_metrics,
        'model_type': 'Transformer (Generation)',
        'model_name': model_name,
        'num_examples': len(all_predictions)
    }

def test_transformer_with_generation(model_name, test_loader, device, max_length=20, num_examples=50):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
    print("\n" + "="*60)
    print("ü§ñ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï TRANSFORMER –ú–û–î–ï–õ–ò (–†–ï–ñ–ò–ú –ì–ï–ù–ï–†–ê–¶–ò–ò)")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
    # tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model.eval()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    transformer_data = prepare_transformer_dataset_from_loader(
        test_loader, tokenizer, max_length=max_length, num_examples=num_examples
    )
    
    if not transformer_data:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞")
        return None
    
    all_predictions = []
    all_references = []
    all_contexts = []
    
    print(f"üß™ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ {len(transformer_data)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
    
    with torch.no_grad():
        for i, example in enumerate(transformer_data):
            input_ids = example['input_ids'].unsqueeze(0).to(device)
            attention_mask = example['attention_mask'].unsqueeze(0).to(device)
            target = example['labels'].to(device)
            
            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ
                generated = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=input_ids.shape[1] + 1,  # +1 —Ç–æ–∫–µ–Ω
                    num_return_sequences=1,
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                context_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
                generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
                continuation = generated_text[len(context_text):].strip()
                
                true_token_text = tokenizer.decode(target.cpu(), skip_special_tokens=True)
                
                if true_token_text.strip() and continuation.strip():
                    all_contexts.append(context_text)
                    all_references.append(true_token_text)
                    all_predictions.append(continuation)
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä
                    if i >= 0: #'{context_text}
                        print(f"\nüîç –ü—Ä–∏–º–µ—Ä '{i} –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
                        print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç: '{context_text}'")
                        print(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: '{true_token_text}'")
                        print(f"   –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{continuation}'")
                        
            except Exception as e:
                continue
            
            if (i + 1) % 10 == 0:
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(transformer_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –í—ã—á–∏—Å–ª—è–µ–º ROUGE –º–µ—Ç—Ä–∏–∫–∏
    rouge_metrics = None
    if all_predictions and all_references:
        try:
            import evaluate
            rouge = evaluate.load('rouge')
            rouge_results = rouge.compute(
                predictions=all_predictions,
                references=all_references,
                use_stemmer=True,
                use_aggregator=True
            )
            
            rouge_metrics = {
                'rouge1': round(rouge_results['rouge1'], 4),
                'rouge2': round(rouge_results['rouge2'], 4),
                'rougeL': round(rouge_results['rougeL'], 4),
                'num_examples': len(all_references)
            }
            
            print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ({model_name}):")
            print(f"   ROUGE-1: {rouge_metrics['rouge1']:.4f}")
            print(f"   ROUGE-2: {rouge_metrics['rouge2']:.4f}")
            print(f"   ROUGE-L: {rouge_metrics['rougeL']:.4f}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ ROUGE: {e}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
    examples_to_show = min(3, len(all_predictions))
    for i in range(examples_to_show):
        print(f"   –ü—Ä–∏–º–µ—Ä {i+1}:")
        print(f"      –ö–æ–Ω—Ç–µ–∫—Å—Ç: '{all_contexts[i]}'")
        print(f"      –û–∂–∏–¥–∞–ª–æ—Å—å: '{all_references[i]}'")
        print(f"      –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{all_predictions[i]}'")
        print()
    
    # –í—ã—á–∏—Å–ª—è–µ–º accuracy –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤)
    accuracy = 0
    if all_predictions and all_references:
        correct = sum(1 for pred, ref in zip(all_predictions, all_references) if pred == ref)
        accuracy = correct / len(all_references)
        print(f"üìä Accuracy –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ): {accuracy:.4f}")
    
    return {
        'accuracy': accuracy,
        'rouge_metrics': rouge_metrics,
        'model_type': 'Transformer',
        'model_name': model_name
    }

def test_transformer_model_new__(model_name, texts_df, device, max_length=20, num_examples=100):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –º–æ–¥–µ–ª—å (DistilGPT2) –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ"""
    print("\n" + "="*60)
    print("ü§ñ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï TRANSFORMER –ú–û–î–ï–õ–ò (DistilGPT2)")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)
    
    # –î–æ–±–∞–≤–ª—è–µ–º pad token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model.eval()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    # transformer_data = prepare_transformer_dataset(texts_df, tokenizer, max_length, num_examples)
    training_pairs= prepare_training_pairs(texts_df, tokenizer, max_length)
    
    if not training_pairs:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞")
        return None
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
    if len(training_pairs) > num_examples:
        training_pairs = training_pairs[:num_examples]
        print(f"üìù –ò—Å–ø–æ–ª—å–∑—É–µ–º {num_examples} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {len(training_pairs)}")
    
    
    total_correct = 0
    total_tokens = 0
    all_predictions = []
    all_references = []
    
    with torch.no_grad():
        for i, (x_padded, y_padded, attention_mask) in enumerate(training_pairs):
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã
            input_ids = torch.tensor(x_padded).unsqueeze(0).to(device)
            attention_mask_tensor = torch.tensor(attention_mask).unsqueeze(0).to(device)
            targets = torch.tensor(y_padded).to(device)
            # input_ids = example['input_ids'].unsqueeze(0).to(device)  # Add batch dimension
            # attention_mask = example['attention_mask'].unsqueeze(0).to(device)
            # target = example['labels'].to(device)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            outputs = model(input_ids, attention_mask=attention_mask_tensor)
            logits = outputs.logits
            
            # –ë–µ—Ä–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
            # next_token_logits = logits[:, -1, :]
            predicted_tokens = torch.argmax(logits, dim=-1).squeeze(0)
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–∞–¥–¥–∏–Ω–≥)
            for j in range(len(targets)):
                if attention_mask[j] == 1:  # –¢–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                    if predicted_tokens[j].item() == targets[j].item():
                        total_correct += 1
                    total_tokens += 1
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—Ç–æ–ª—å–∫–æ –Ω–µ-pad —Ç–æ–∫–µ–Ω—ã)
                    try:
                        true_token_text = tokenizer.decode([targets[j].item()], skip_special_tokens=True)
                        pred_token_text = tokenizer.decode([predicted_tokens[j].item()], skip_special_tokens=True)
                        
                        if true_token_text.strip() and pred_token_text.strip():
                            all_references.append(true_token_text)
                            all_predictions.append(pred_token_text)
                    except Exception as e:
                        continue
            
            if (i + 1) % 50 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(training_pairs)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            try:
                true_token_text = tokenizer.decode(targets.cpu(), skip_special_tokens=True)
                pred_token_text = tokenizer.decode(predicted_tokens.cpu(), skip_special_tokens=True)
                
                if true_token_text.strip() and pred_token_text.strip():
                    all_references.append(true_token_text)
                    all_predictions.append(pred_token_text)
            except Exception as e:
                continue
            
            if (i + 1) % 50 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(training_pairs)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –í—ã—á–∏—Å–ª—è–µ–º accuracy
    accuracy = total_correct / total_tokens if total_tokens > 0 else 0
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Transformer ({model_name}):")
    print(f"   Test Accuracy: {accuracy:.4f}")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}")
    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {total_correct}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º ROUGE –º–µ—Ç—Ä–∏–∫–∏
    rouge_metrics = None
    if all_predictions and all_references:
        try:
            
            rouge = evaluate.load('rouge')
            rouge_results = rouge.compute(
                predictions=all_predictions,
                references=all_references,
                use_stemmer=True,
                use_aggregator=True
            )
            
            rouge_metrics = {
                'rouge1': round(rouge_results['rouge1'], 4),
                'rouge2': round(rouge_results['rouge2'], 4),
                'rougeL': round(rouge_results['rougeL'], 4),
                'rougeLsum': round(rouge_results['rougeLsum'], 4),
                'num_examples': len(all_references)
            }
            
            print(f"   ROUGE-1: {rouge_metrics['rouge1']:.4f}")
            print(f"   ROUGE-2: {rouge_metrics['rouge2']:.4f}")
            print(f"   ROUGE-L: {rouge_metrics['rougeL']:.4f}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ ROUGE: {e}")
            rouge_metrics = None
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ROUGE –º–µ—Ç—Ä–∏–∫")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π Transformer:")
    examples_to_show = min(100, len(all_predictions))
    for i in range(examples_to_show):
        print(f"   –ü—Ä–∏–º–µ—Ä {i+1}:")
        print(f"      –û–∂–∏–¥–∞–ª–æ—Å—å: '{all_references[i]}'")
        print(f"      –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: '{all_predictions[i]}'")
        status = "‚úÖ" if all_references[i] == all_predictions[i] else "‚ùå"
        print(f"      –°—Ç–∞—Ç—É—Å: {status}")
        print()
    
    return {
        'accuracy': accuracy,
        'rouge_metrics': rouge_metrics,
        'model_type': 'Transformer',
        'model_name': model_name
    }

def test_transformer_model_new(model_name, texts_df, device, max_length=20, num_examples=100):
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
    print("\n" + "="*60)
    print("ü§ñ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï TRANSFORMER –ú–û–î–ï–õ–ò")
    print("="*60)
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model.eval()
    
    training_pairs = prepare_training_pairs(texts_df, tokenizer, max_length)
    
    if not training_pairs:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        return None
    
    training_pairs = training_pairs[:num_examples]
    
    total_correct = 0
    total_tokens = 0
    
    print(f"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ {len(training_pairs)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
    
    with torch.no_grad():
        for i, (x_padded, y_padded, attention_mask) in enumerate(training_pairs):
            input_ids = torch.tensor(x_padded).unsqueeze(0).to(device)
            attention_mask_tensor = torch.tensor(attention_mask).unsqueeze(0).to(device)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            non_pad_indices = (attention_mask_tensor[0] == 1).nonzero(as_tuple=True)[0]
            
            if len(non_pad_indices) < 2:
                continue
            
            targets = torch.tensor(y_padded).to(device)
            
            try:
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
                generated = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask_tensor,
                    max_length=input_ids.shape[1] + len(non_pad_indices),
                    num_return_sequences=1,
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
                
                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
                generated_sequence = generated[0, input_ids.shape[1]:]
                
                for j in range(min(len(generated_sequence), len(non_pad_indices))):
                    generated_token = generated_sequence[j]
                    target_token = targets[j]
                    
                    if generated_token.item() == target_token.item():
                        total_correct += 1
                    total_tokens += 1
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ {i}: {e}")
                continue
            
            if (i + 1) % 10 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(training_pairs)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    accuracy = total_correct / total_tokens if total_tokens > 0 else 0
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"   Accuracy: {accuracy:.4f}")
    print(f"   –¢–æ–∫–µ–Ω–æ–≤: {total_tokens}")
    # –í—ã—á–∏—Å–ª—è–µ–º ROUGE –º–µ—Ç—Ä–∏–∫–∏
    rouge_metrics = None
    if all_predictions and all_references:
        try:
            import evaluate
            rouge = evaluate.load('rouge')
            rouge_results = rouge.compute(
                predictions=all_predictions,
                references=all_references,
                use_stemmer=True,
                use_aggregator=True
            )
            
            rouge_metrics = {
                'rouge1': round(rouge_results['rouge1'], 4),
                'rouge2': round(rouge_results['rouge2'], 4),
                'rougeL': round(rouge_results['rougeL'], 4),
                'rougeLsum': round(rouge_results['rougeLsum'], 4),
                'num_examples': len(all_references)
            }
            
            print(f"   ROUGE-1: {rouge_metrics['rouge1']:.4f}")
            print(f"   ROUGE-2: {rouge_metrics['rouge2']:.4f}")
            print(f"   ROUGE-L: {rouge_metrics['rougeL']:.4f}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ ROUGE: {e}")
            rouge_metrics = None
    
    return {
        'accuracy': accuracy,
        'rouge_metrics': rouge_metrics,
        'model_type': 'Transformer',
        'model_name': model_name
    }

def evaluate_transformer_forward(model_name, data, device, compute_rouge=False, num_rouge_examples=50):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π forward pass"""
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)
    model.eval()
    
    
    _, data_loader = dataset_preparation(data, tokenizer, MAX_LEN=20, batch_size=128)
    
    preds, trues = [], []
    total_loss = 0
    
    # –î–ª—è ROUGE –º–µ—Ç—Ä–∏–∫
    references, predictions = [], []
    
    print(f"üß™ –û—Ü–µ–Ω–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ (forward) –Ω–∞ {len(data_loader)} –±–∞—Ç—á–∞—Ö...")
    
    with torch.no_grad():
        examples_processed = 0
        for batch_idx, batch in enumerate(data_loader):
            ids = batch['data'].to(device)
            mask = batch['mask'].to(device)
            labels = batch['target'].to(device)
            
            # –ü—Ä—è–º–æ–π forward pass
            outputs = model(
                input_ids=ids,
                attention_mask=mask,
                labels=labels  # –î–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è loss
            )
            
            logits = outputs.logits
            loss = outputs.loss
            
            total_loss += loss.item()
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
            batch_preds = torch.argmax(logits, dim=-1)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            preds.extend(batch_preds.cpu().flatten().tolist())
            trues.extend(labels.cpu().flatten().tolist())
            
            # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è ROUGE
            if compute_rouge and examples_processed < num_rouge_examples:
                for i in range(ids.size(0)):
                    if examples_processed >= num_rouge_examples:
                        break
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–∞–¥–¥–∏–Ω–≥)
                    non_pad_indices = mask[i].bool()
                    if non_pad_indices.sum() == 0:
                        continue
                    
                    input_ids_real = ids[i][non_pad_indices]
                    preds_real = batch_preds[i][non_pad_indices]
                    labels_real = labels[i][non_pad_indices]
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–∫—Å—Ç
                    try:
                        # –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç (–∫–æ–Ω—Ç–µ–∫—Å—Ç)
                        input_tokens = tokenizer.convert_ids_to_tokens(input_ids_real.cpu())
                        input_text = tokenizer.convert_tokens_to_string(input_tokens)
                        
                        # –¶–µ–ª–µ–≤–æ–π —Ç–µ–∫—Å—Ç (–æ–∂–∏–¥–∞–µ–º–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ)
                        true_tokens = tokenizer.convert_ids_to_tokens(labels_real.cpu())
                        true_text = tokenizer.convert_tokens_to_string(true_tokens)
                        
                        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                        pred_tokens = tokenizer.convert_ids_to_tokens(preds_real.cpu())
                        pred_text = tokenizer.convert_tokens_to_string(pred_tokens)
                        
                        if true_text.strip() and pred_text.strip():
                            references.append(true_text)
                            predictions.append(pred_text)
                            examples_processed += 1
                            
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–∏–º–µ—Ä–∞ –¥–ª—è ROUGE: {e}")
                        continue
            
            if (batch_idx + 1) % 10 == 0:
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±–∞—Ç—á–µ–π: {batch_idx + 1}/{len(data_loader)}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(trues, preds) if trues and preds else 0
    avg_loss = total_loss / len(data_loader)
    
    print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ (forward):")
    print(f"   Accuracy: {accuracy:.4f}")
    print(f"   Loss: {avg_loss:.4f}")
    print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤: {len(preds)}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º ROUGE –º–µ—Ç—Ä–∏–∫–∏
    rouge_metrics = None
    if compute_rouge and references and predictions:
        try:
            import evaluate
            rouge = evaluate.load('rouge')
            rouge_results = rouge.compute(
                predictions=predictions,
                references=references,
                use_stemmer=True,
                use_aggregator=True
            )
            
            rouge_metrics = {
                'rouge1': round(rouge_results['rouge1'], 4),
                'rouge2': round(rouge_results['rouge2'], 4),
                'rougeL': round(rouge_results['rougeL'], 4),
                'rougeLsum': round(rouge_results['rougeLsum'], 4),
                'num_examples': len(references)
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ ROUGE: {e}")
            rouge_metrics = {'error': str(e)}
    
    return accuracy, avg_loss, rouge_metrics


def compare_models(results_lstm, results_transformer):
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π"""
    print("\n" + "="*60)
    print("üìà –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("="*60)
    
    if results_lstm:
        print(f"\nüß† {results_lstm['model_type']}:")
        print(f"   Accuracy: {results_lstm['accuracy']:.4f}")
        if results_lstm.get('rouge_metrics'):
            print(f"   ROUGE-1: {results_lstm['rouge_metrics']['rouge1']:.4f}")
    else:
        print(f"\nüß† LSTM: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    if results_transformer:
        print(f"\nü§ñ {results_transformer['model_type']} ({results_transformer['model_name']}):")
        print(f"   Accuracy: {results_transformer['accuracy']:.4f}")
        if results_transformer.get('rouge_metrics'):
            print(f"   ROUGE-1: {results_transformer['rouge_metrics']['rouge1']:.4f}")
    else:
        print(f"\nü§ñ Transformer: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ accuracy
    if results_lstm and results_transformer:
        if results_lstm['accuracy'] > results_transformer['accuracy']:
            winner = "LSTM"
            diff = results_lstm['accuracy'] - results_transformer['accuracy']
        else:
            winner = "Transformer"
            diff = results_transformer['accuracy'] - results_lstm['accuracy']
        
        print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ accuracy: {winner}")
        print(f"   –†–∞–∑–Ω–∏—Ü–∞: {diff:.4f}")
    else:
        print(f"\nüèÜ –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å - –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö")

def main():
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º –∏ –º–æ–¥–µ–ª–∏
    data_path = Path(__file__).parent.parent / 'data' / '1_raw_dataset_tweets.txt'
    model_path = Path(__file__).parent.parent / 'models' / 'best_model.pth'
    
    # 1) –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É –Ω–∞ 1000 –∑–∞–ø–∏—Å–µ–π
    test_loader, lstm_tokenizer, texts_df = create_test_dataset(
        file_path=data_path, 
        limit=1000,
        max_len=20
    )
    
    # 2) –¢–µ—Å—Ç–∏—Ä—É–µ–º LSTM –º–æ–¥–µ–ª—å
    results_lstm = None
    if model_path.exists():
        try:
            results_lstm = test_lstm_model(model_path, test_loader, lstm_tokenizer, device)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ LSTM –º–æ–¥–µ–ª–∏: {e}")
    else:
        print(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ LSTM –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
    
    # 3) –¢–µ—Å—Ç–∏—Ä—É–µ–º Transformer –º–æ–¥–µ–ª—å (DistilGPT2)
    transformer_model_name = "distilgpt2"
    try:
        # results_transformer = test_transformer_model(
        #     model_name=transformer_model_name,
        #     texts_df=texts_df,
        #     device=device,
        #     max_length=20,
        #     num_examples=200  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        # )
        results_transformer = test_transformer_with_generation(
            model_name=transformer_model_name, 
            test_loader=test_loader, 
            device=device, 
            max_length=20, 
            num_examples=200
        )
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ Transformer –º–æ–¥–µ–ª–∏: {e}")
        results_transformer = None
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    compare_models(results_lstm, results_transformer)
    
    print(f"\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == "__main__":
    main()