import torch
from torch.utils.data import DataLoader
from pathlib import Path
from transformers import AutoTokenizer, GPT2LMHeadModel
import random
from data_utils import load_and_clean_data, prepare_training_pairs
from next_token_dataset import TweetsDataset
from lstm_model import NextPhrasePredictionRNN
from eval_lstm import vevaluate, test_model, analyze_predictions, show_detailed_examples
from sklearn.model_selection import train_test_split

def create_test_dataset(file_path, limit=1000, max_len=20):
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É –Ω–∞ 1000 –∑–∞–ø–∏—Å–µ–π"""
    print("üìä –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    texts_df = load_and_clean_data(file_path, limit)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º BertTokenizer –¥–ª—è LSTM –º–æ–¥–µ–ª–∏
    lstm_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM
    data = prepare_training_pairs(texts_df, lstm_tokenizer, max_len)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–Ω–µ —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test)
    test_data = data
    
    X_test, Y_test, M_test = zip(*test_data)
    
    # –°–æ–∑–¥–∞–µ–º dataset –∏ loader –¥–ª—è LSTM
    test_ds = TweetsDataset(X_test, Y_test, M_test)
    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)
    
    print(f"‚úÖ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∞: {len(test_ds)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    return test_loader, lstm_tokenizer, texts_df

def test_lstm_model(model_path, test_loader, tokenizer, device):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç LSTM –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ"""
    print("\n" + "="*60)
    print("üß† –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï LSTM –ú–û–î–ï–õ–ò")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
    checkpoint = torch.load(model_path, map_location=device)
    model_config = checkpoint['model_config']
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = NextPhrasePredictionRNN(
        rnn_type="LSTM",
        vocab_size=model_config['vocab_size'],
        emb_dim=model_config['emb_dim'],
        hidden_dim=model_config['hidden_dim'],
        pad_idx=model_config['pad_idx']
    ).to(device)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    criterion = torch.nn.CrossEntropyLoss(ignore_index=model_config['pad_idx'])
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    test_accuracy, test_loss = test_model(model, test_loader, criterion, device)
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å ROUGE –º–µ—Ç—Ä–∏–∫–∞–º–∏
    accuracy, avg_loss, rouge_metrics = vevaluate(
        model, test_loader, criterion, device, tokenizer, 
        compute_rouge=True, num_rouge_examples=100
    )
    
    print(f"\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã LSTM:")
    print(f"   Test Accuracy: {accuracy:.4f}")
    print(f"   Test Loss: {avg_loss:.4f}")
    if rouge_metrics and 'rouge1' in rouge_metrics:
        print(f"   ROUGE-1: {rouge_metrics['rouge1']:.4f}")
        print(f"   ROUGE-2: {rouge_metrics['rouge2']:.4f}")
        print(f"   ROUGE-L: {rouge_metrics['rougeL']:.4f}")
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    bad_cases, good_cases = analyze_predictions(model, test_loader, tokenizer, device)
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    show_detailed_examples(model, test_loader, tokenizer, num_examples=3)
    
    return {
        'accuracy': accuracy,
        'loss': avg_loss,
        'rouge_metrics': rouge_metrics,
        'model_type': 'LSTM'
    }

def prepare_transformer_dataset(texts_df, tokenizer, max_length=20, num_examples=100):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ next-token prediction"""
    print("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞...")
    
    data = []
    for text in texts_df['text_cleaned'][:num_examples]:
        if len(text.strip()) < 5:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
            continue
            
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        inputs = tokenizer(text, truncation=True, max_length=max_length, return_tensors="pt")
        input_ids = inputs['input_ids'][0]
        
        if len(input_ids) < 2:  # –ù—É–∂–µ–Ω –∫–∞–∫ –º–∏–Ω–∏–º—É–º 1 —Ç–æ–∫–µ–Ω –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ 1 –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            continue
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã next-token prediction
        for i in range(len(input_ids) - 1):
            context = input_ids[:i+1]
            target = input_ids[i+1]
            
            # –ü–∞–¥–¥–∏–Ω–≥ –¥–æ max_length
            if len(context) < max_length:
                padding = torch.full((max_length - len(context),), tokenizer.pad_token_id)
                context = torch.cat([padding, context])
            else:
                context = context[-max_length:]
            
            attention_mask = (context != tokenizer.pad_token_id).long()
            
            data.append({
                'input_ids': context,
                'attention_mask': attention_mask,
                'labels': target.unsqueeze(0)  # –î–æ–±–∞–≤–ª—è–µ–º dimension –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            })
    
    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞")
    return data

def test_transformer_model(model_name, texts_df, device, max_length=20, num_examples=100):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –º–æ–¥–µ–ª—å (DistilGPT2) –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ"""
    print("\n" + "="*60)
    print("ü§ñ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï TRANSFORMER –ú–û–î–ï–õ–ò (DistilGPT2)")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)
    
    # –î–æ–±–∞–≤–ª—è–µ–º pad token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model.eval()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    transformer_data = prepare_transformer_dataset(texts_df, tokenizer, max_length, num_examples)
    
    if not transformer_data:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞")
        return None
    
    total_correct = 0
    total_tokens = 0
    all_predictions = []
    all_references = []
    
    with torch.no_grad():
        for i, example in enumerate(transformer_data):
            input_ids = example['input_ids'].unsqueeze(0).to(device)  # Add batch dimension
            attention_mask = example['attention_mask'].unsqueeze(0).to(device)
            target = example['labels'].to(device)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            
            # –ë–µ—Ä–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
            next_token_logits = logits[:, -1, :]
            predicted_token = torch.argmax(next_token_logits, dim=-1)
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —Ü–µ–ª–µ–≤—ã–º —Ç–æ–∫–µ–Ω–æ–º
            if predicted_token.item() == target.item():
                total_correct += 1
            total_tokens += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            try:
                true_token_text = tokenizer.decode(target.cpu(), skip_special_tokens=True)
                pred_token_text = tokenizer.decode(predicted_token.cpu(), skip_special_tokens=True)
                
                if true_token_text.strip() and pred_token_text.strip():
                    all_references.append(true_token_text)
                    all_predictions.append(pred_token_text)
            except Exception as e:
                continue
            
            if (i + 1) % 50 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(transformer_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –í—ã—á–∏—Å–ª—è–µ–º accuracy
    accuracy = total_correct / total_tokens if total_tokens > 0 else 0
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Transformer ({model_name}):")
    print(f"   Test Accuracy: {accuracy:.4f}")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}")
    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {total_correct}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º ROUGE –º–µ—Ç—Ä–∏–∫–∏
    rouge_metrics = None
    if all_predictions and all_references:
        try:
            import evaluate
            rouge = evaluate.load('rouge')
            rouge_results = rouge.compute(
                predictions=all_predictions,
                references=all_references,
                use_stemmer=True,
                use_aggregator=True
            )
            
            rouge_metrics = {
                'rouge1': round(rouge_results['rouge1'], 4),
                'rouge2': round(rouge_results['rouge2'], 4),
                'rougeL': round(rouge_results['rougeL'], 4),
                'rougeLsum': round(rouge_results['rougeLsum'], 4),
                'num_examples': len(all_references)
            }
            
            print(f"   ROUGE-1: {rouge_metrics['rouge1']:.4f}")
            print(f"   ROUGE-2: {rouge_metrics['rouge2']:.4f}")
            print(f"   ROUGE-L: {rouge_metrics['rougeL']:.4f}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ ROUGE: {e}")
            rouge_metrics = None
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ROUGE –º–µ—Ç—Ä–∏–∫")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π Transformer:")
    examples_to_show = min(5, len(all_predictions))
    for i in range(examples_to_show):
        print(f"   –ü—Ä–∏–º–µ—Ä {i+1}:")
        print(f"      –û–∂–∏–¥–∞–ª–æ—Å—å: '{all_references[i]}'")
        print(f"      –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: '{all_predictions[i]}'")
        status = "‚úÖ" if all_references[i] == all_predictions[i] else "‚ùå"
        print(f"      –°—Ç–∞—Ç—É—Å: {status}")
        print()
    
    return {
        'accuracy': accuracy,
        'rouge_metrics': rouge_metrics,
        'model_type': 'Transformer',
        'model_name': model_name
    }

def compare_models(results_lstm, results_transformer):
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π"""
    print("\n" + "="*60)
    print("üìà –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("="*60)
    
    if results_lstm:
        print(f"\nüß† {results_lstm['model_type']}:")
        print(f"   Accuracy: {results_lstm['accuracy']:.4f}")
        if results_lstm.get('rouge_metrics'):
            print(f"   ROUGE-1: {results_lstm['rouge_metrics']['rouge1']:.4f}")
    else:
        print(f"\nüß† LSTM: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    if results_transformer:
        print(f"\nü§ñ {results_transformer['model_type']} ({results_transformer['model_name']}):")
        print(f"   Accuracy: {results_transformer['accuracy']:.4f}")
        if results_transformer.get('rouge_metrics'):
            print(f"   ROUGE-1: {results_transformer['rouge_metrics']['rouge1']:.4f}")
    else:
        print(f"\nü§ñ Transformer: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ accuracy
    if results_lstm and results_transformer:
        if results_lstm['accuracy'] > results_transformer['accuracy']:
            winner = "LSTM"
            diff = results_lstm['accuracy'] - results_transformer['accuracy']
        else:
            winner = "Transformer"
            diff = results_transformer['accuracy'] - results_lstm['accuracy']
        
        print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ accuracy: {winner}")
        print(f"   –†–∞–∑–Ω–∏—Ü–∞: {diff:.4f}")
    else:
        print(f"\nüèÜ –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å - –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö")

def main():
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º –∏ –º–æ–¥–µ–ª–∏
    data_path = Path(__file__).parent.parent / 'data' / '1_raw_dataset_tweets.txt'
    model_path = Path(__file__).parent.parent / 'models' / 'best_model.pth'
    
    # 1) –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É –Ω–∞ 1000 –∑–∞–ø–∏—Å–µ–π
    test_loader, lstm_tokenizer, texts_df = create_test_dataset(
        file_path=data_path, 
        limit=1000,
        max_len=20
    )
    
    # 2) –¢–µ—Å—Ç–∏—Ä—É–µ–º LSTM –º–æ–¥–µ–ª—å
    results_lstm = None
    if model_path.exists():
        try:
            results_lstm = test_lstm_model(model_path, test_loader, lstm_tokenizer, device)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ LSTM –º–æ–¥–µ–ª–∏: {e}")
    else:
        print(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ LSTM –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
    
    # 3) –¢–µ—Å—Ç–∏—Ä—É–µ–º Transformer –º–æ–¥–µ–ª—å (DistilGPT2)
    transformer_model_name = "distilgpt2"
    try:
        results_transformer = test_transformer_model(
            model_name=transformer_model_name,
            texts_df=texts_df,
            device=device,
            max_length=20,
            num_examples=200  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        )
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ Transformer –º–æ–¥–µ–ª–∏: {e}")
        results_transformer = None
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    compare_models(results_lstm, results_transformer)
    
    print(f"\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == "__main__":
    main()