def compare_models_lstm_vs_gpt2():
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ LSTM –∏ distilGPT2 –º–æ–¥–µ–ª–µ–π —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–ª–∏–Ω—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
    current_dir = Path.cwd()
    v_file_path = current_dir / 'data' / 'test_dataset.csv'
    test_df = load_samples(v_file_path)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # LSTM –º–æ–¥–µ–ª—å 1
    model_path = current_dir / 'models' / 'lstm_50K_best_model.pth'
    lstm_tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
    checkpoint = torch.load(model_path, map_location=device)
    model_config = checkpoint['model_config']
    
    lstm_model = NextPhrasePredictionRNN(
        rnn_type="LSTM",
        vocab_size=model_config['vocab_size'],
        emb_dim=model_config['emb_dim'],
        hidden_dim=model_config['hidden_dim'],
        pad_idx=model_config['pad_idx']
    ).to(device)
    lstm_model.load_state_dict(checkpoint['model_state_dict'])
    lstm_model.eval()
    
    # GRU –º–æ–¥–µ–ª—å 2
    model_path2 = current_dir / 'models' / 'gru_MaxL10NumEp100Lim500000NumL2Hidd128Emb300Lr1E-4WDec005_best_model.pth'
    # lstm_tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
    checkpoint2 = torch.load(model_path2, map_location=device)
    model_config2 = checkpoint2['model_config']
    
    lstm_model2 = NextPhrasePredictionRNN(
        rnn_type="GRU",
        vocab_size=model_config2['vocab_size'],
        emb_dim=model_config2['emb_dim'],
        hidden_dim=model_config2['hidden_dim'],
        pad_idx=model_config2['pad_idx'],
        num_layers=model_config2['num_layers']
    ).to(device)
    lstm_model2.load_state_dict(checkpoint2['model_state_dict'])
    lstm_model2.eval()
    
    
    # distilGPT2 –º–æ–¥–µ–ª—å
    gpt2_tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
    if gpt2_tokenizer.pad_token is None:
        gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token
    
    gpt2_model = GPT2LMHeadModel.from_pretrained("distilgpt2").to(device)
    gpt2_model.eval()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ ROUGE –º–µ—Ç—Ä–∏–∫–∏
    rouge = evaluate.load('rouge')
    
    def calculate_expected_tokens_count(original_phrase, truncated_phrase, tokenizer):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–∂–∏–¥–∞–µ–º–æ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–∏"""
        full_tokens = tokenizer.tokenize(original_phrase)
        truncated_tokens = tokenizer.tokenize(truncated_phrase)
        expected_tokens_count = len(full_tokens) - len(truncated_tokens)
        return max(expected_tokens_count, 1)  # –º–∏–Ω–∏–º—É–º 1 —Ç–æ–∫–µ–Ω
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è LSTM —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–ª–∏–Ω—ã
    def generate_lstm_completion(lstm_model, prompt_text, expected_tokens_count):
        start_time = time.time()
        
        inputs = lstm_tokenizer(prompt_text, return_tensors='pt', truncation=True, max_length=50)
        input_ids = inputs['input_ids'].to(device)
        attention_mask = inputs['attention_mask'].to(device)
        
        current_input = input_ids.clone()
        current_mask = attention_mask.clone()
        generated_tokens = []
        
        with torch.no_grad():
            for step in range(expected_tokens_count):
                logits = lstm_model(current_input, current_mask)
                next_token_logits = logits[0, -1, :]
                next_token = torch.argmax(next_token_logits, dim=-1)
                next_token_id = next_token.item()
                
                generated_tokens.append(next_token_id)
                next_token_tensor = next_token.unsqueeze(0).unsqueeze(0)
                current_input = torch.cat([current_input, next_token_tensor], dim=1)
                
                new_mask = torch.ones(1, 1, device=device, dtype=torch.long)
                current_mask = torch.cat([current_mask, new_mask], dim=1)
                
                # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ EOS —Ç–æ–∫–µ–Ω–µ
                if next_token_id == lstm_tokenizer.eos_token_id:
                    break
        
        completion = lstm_tokenizer.decode(generated_tokens, skip_special_tokens=True)
        end_time = time.time()
        generation_time = end_time - start_time
        
        return completion, generation_time, len(generated_tokens)
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è GPT2 —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–ª–∏–Ω—ã
    def generate_gpt2_completion(prompt_text, expected_tokens_count):
        start_time = time.time()
        
        inputs = gpt2_tokenizer(prompt_text, return_tensors='pt', truncation=True, max_length=50)
        input_ids = inputs['input_ids'].to(device)
        attention_mask = inputs['attention_mask'].to(device)
        
        with torch.no_grad():
            outputs = gpt2_model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_length=input_ids.shape[1] + expected_tokens_count,
                num_return_sequences=1,
                do_sample=False,  # greedy decoding
                pad_token_id=gpt2_tokenizer.pad_token_id,
                eos_token_id=gpt2_tokenizer.eos_token_id,
                early_stopping=False,
            )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
        generated_tokens = outputs[0][input_ids.shape[1]:]
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤
        if len(generated_tokens) > expected_tokens_count:
            generated_tokens = generated_tokens[:expected_tokens_count]
        
        completion = gpt2_tokenizer.decode(generated_tokens, skip_special_tokens=True)
        end_time = time.time()
        generation_time = end_time - start_time
        
        return completion, generation_time, len(generated_tokens)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    results = []
    
    print("üîç –ù–∞—á–∞–ª–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...")
    print(f"üìä –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # for idx, row in test_df.iterrows():
    progress_bar = tqdm(test_df.iterrows(), total=len(test_df), desc="üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤")
    for idx, row in progress_bar:
        if idx>=200:
            break
        progress_bar.set_postfix({
        "–ü—Ä–∏–º–µ—Ä": f"{idx + 1}/{len(test_df)}"
        # ,
        # "–¢–µ–∫—Å—Ç": f"{row['text_cleaned'][:30]}..." if len(str(row['text_cleaned'])) > 30 else str(row['text_cleaned'])
    })
        
        original_phrase = row['text_cleaned']
        
        # –°–æ–∑–¥–∞–µ–º —É—Ä–µ–∑–∞–Ω–Ω—É—é —Ñ—Ä–∞–∑—É (–ø–µ—Ä–≤—ã–µ 5-7 —Å–ª–æ–≤)
        words = original_phrase.split()
        if len(words) > 7:
            truncated_phrase = ' '.join(words[:7])
        else:
            truncated_phrase = ' '.join(words[:-1]) if len(words) > 1 else original_phrase
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–∂–∏–¥–∞–µ–º–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ
        expected_continuation = original_phrase[len(truncated_phrase):].strip()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–∂–∏–¥–∞–µ–º–æ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        lstm_expected_tokens = calculate_expected_tokens_count(original_phrase, truncated_phrase, lstm_tokenizer)
        gpt2_expected_tokens = calculate_expected_tokens_count(original_phrase, truncated_phrase, gpt2_tokenizer)
        
        # if idx<5:
        #     tqdm.write(f"\nüìù –ü—Ä–∏–º–µ—Ä {idx + 1}/{len(test_df)}")
        #     tqdm.write(f"   –û—Ä–∏–≥–∏–Ω–∞–ª: '{original_phrase}'")
        #     tqdm.write(f"   –£—Ä–µ–∑–∞–Ω–æ: '{truncated_phrase}'")
        #     tqdm.write(f"   –û–∂–∏–¥–∞–µ–º–æ–µ: '{expected_continuation}'")
        #     tqdm.write(f"   –û–∂–∏–¥–∞–µ–º–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ - LSTM: {lstm_expected_tokens}, GPT2: {gpt2_expected_tokens}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–π —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–ª–∏–Ω—ã
        lstm_completion, lstm_time, lstm_actual_tokens = generate_lstm_completion(lstm_model, truncated_phrase, lstm_expected_tokens)
        lstm_completion2, lstm_time2, lstm_actual_tokens2 = generate_lstm_completion(lstm_model2, truncated_phrase, lstm_expected_tokens)
        gpt2_completion, gpt2_time, gpt2_actual_tokens = generate_gpt2_completion(truncated_phrase, gpt2_expected_tokens)
        
        # if idx<5:
        #     tqdm.write(f"   LSTM: '{lstm_completion}' ({lstm_actual_tokens} —Ç–æ–∫–µ–Ω–æ–≤, {lstm_time:.2f}—Å)")
        #     tqdm.write(f"   GRU: '{lstm_completion2}' ({lstm_actual_tokens2} —Ç–æ–∫–µ–Ω–æ–≤, {lstm_time2:.2f}—Å)")
        #     tqdm.write(f"   GPT2: '{gpt2_completion}' ({gpt2_actual_tokens} —Ç–æ–∫–µ–Ω–æ–≤, {gpt2_time:.2f}—Å)")
        
        # –í—ã—á–∏—Å–ª—è–µ–º ROUGE –º–µ—Ç—Ä–∏–∫–∏
        lstm_rouge = rouge.compute(
            predictions=[lstm_completion],
            references=[expected_continuation],
            use_stemmer=True
        ) if lstm_completion.strip() and expected_continuation.strip() else {'rougeL': 0.0}

        lstm_rouge2 = rouge.compute(
            predictions=[lstm_completion2],
            references=[expected_continuation],
            use_stemmer=True
        ) if lstm_completion2.strip() and expected_continuation.strip() else {'rougeL': 0.0}

        
        gpt2_rouge = rouge.compute(
            predictions=[gpt2_completion],
            references=[expected_continuation],
            use_stemmer=True
        ) if gpt2_completion.strip() and expected_continuation.strip() else {'rougeL': 0.0}
        
        results.append({
            '–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Ñ—Ä–∞–∑–∞': original_phrase,
            '–£—Ä–µ–∑–∞–Ω–Ω–∞—è —Ñ—Ä–∞–∑–∞': truncated_phrase,
            '–û–∂–∏–¥–∞–µ–º–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ': expected_continuation,
            # '–û–∂–∏–¥–∞–µ–º–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ LSTM': lstm_expected_tokens,
            # '–û–∂–∏–¥–∞–µ–º–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ GPT2': gpt2_expected_tokens,
            '–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã –º–æ–¥–µ–ª–∏ LSTM': lstm_completion,
            '–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã –º–æ–¥–µ–ª–∏ GRU': lstm_completion2,
            
            # '–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ LSTM': lstm_actual_tokens,
            '–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã –º–æ–¥–µ–ª–∏ distilGPT2': gpt2_completion,
            # '–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ GPT2': gpt2_actual_tokens,
            'rouge LSTM': round(lstm_rouge['rougeL'], 4),
            'rouge GRU': round(lstm_rouge2['rougeL'], 4),
            'rouge distilGPT2': round(gpt2_rouge['rougeL'], 4),
            '–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ LSTM': round(lstm_time, 4),
            '–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ GRU': round(lstm_time2, 4),
            '–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ distilGPT2': round(gpt2_time, 4)
        })
    
    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    results_df = pd.DataFrame(results)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    # output_path = current_dir / 'results' / f'model_comparison_controlled_length_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
    output_path = current_dir / 'results' / 'model_comparison.csv'
    output_path.parent.mkdir(exist_ok=True)
    results_df.to_csv(output_path, index=False, encoding='utf-8')
    
    # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    total_stats = {
        '–°—Ä–µ–¥–Ω–∏–π ROUGE-L LSTM': results_df['rouge LSTM'].mean(),
        '–°—Ä–µ–¥–Ω–∏–π ROUGE-L GRU': results_df['rouge GRU'].mean(),       
        '–°—Ä–µ–¥–Ω–∏–π ROUGE-L GPT2': results_df['rouge distilGPT2'].mean(),
        '–û–±—â–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)': results_df['–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ LSTM'].sum(),
        '–û–±—â–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)': results_df['–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ GRU'].sum(),
        '–û–±—â–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)': results_df['–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ distilGPT2'].sum(),
        '–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)': results_df['–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ LSTM'].mean(),
        '–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è GRU (—Å–µ–∫)': results_df['–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ GRU'].mean(),        
        '–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)': results_df['–≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ distilGPT2'].mean(),
        # '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã LSTM (%)': (results_df['–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ LSTM'] == results_df['–û–∂–∏–¥–∞–µ–º–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ LSTM']).mean() * 100,
        # '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã GPT2 (%)': (results_df['–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ GPT2'] == results_df['–û–∂–∏–¥–∞–µ–º–æ–µ —Ç–æ–∫–µ–Ω–æ–≤ GPT2']).mean() * 100,
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤': len(results_df)
    }
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    # if total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L LSTM'] > total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GPT2']:
    #     best_model = "LSTM"
    #     advantage = total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L LSTM'] - total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GPT2']
    # else:
    #     best_model = "distilGPT2"
    #     advantage = total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GPT2'] - total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L LSTM']
    
    model_scores = {
    'LSTM': total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L LSTM'],
    'GRU': total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GRU'], 
    'distilGPT2': total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GPT2']
    }
    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ —É–±—ã–≤–∞–Ω–∏—é ROUGE-L
    sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)

    
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "="*80)
    print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–†–ê–í–ù–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô")
    print("="*80)
    print(f"ü§ñ LSTM –º–æ–¥–µ–ª—å:")
    print(f"   –°—Ä–µ–¥–Ω–∏–π ROUGE-L: {total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L LSTM']:.4f}")
    # print(f"   –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã: {total_stats['–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã LSTM (%)']:.1f}%")
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)']:.2f} —Å–µ–∫")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: {total_stats['–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)']:.4f} —Å–µ–∫")
    #---------------------------------------------------------------------------------------
    print(f"ü§ñ GRU –º–æ–¥–µ–ª—å:")
    print(f"   –°—Ä–µ–¥–Ω–∏–π ROUGE-L: {total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GRU']:.4f}")
    # print(f"   –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã: {total_stats['–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã LSTM (%)']:.1f}%")
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è GRU (—Å–µ–∫)']:.2f} —Å–µ–∫")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: {total_stats['–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è GRU (—Å–µ–∫)']:.4f} —Å–µ–∫")
    #---------------------------------------------------------------------------------------    
    print(f"\nü§ñ distilGPT2 –º–æ–¥–µ–ª—å:")
    print(f"   –°—Ä–µ–¥–Ω–∏–π ROUGE-L: {total_stats['–°—Ä–µ–¥–Ω–∏–π ROUGE-L GPT2']:.4f}")
    # print(f"   –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã: {total_stats['–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª–∏–Ω—ã GPT2 (%)']:.1f}%")
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)']:.2f} —Å–µ–∫")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: {total_stats['–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)']:.4f} —Å–µ–∫")
    #---------------------------------------------------------------------------------------
    # print(f"\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model}")
    # print(f"   –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –≤ ROUGE-L: {advantage:.4f}")
    print(f"\nüèÜ –†–ï–ô–¢–ò–ù–ì –ú–û–î–ï–õ–ï–ô:")
    [print(f"   {i}. {m}: {s:.4f}" + (f" (–æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ: {sorted_models[0][1]-s:.4f})" if i>1 else "")) 
    for i, (m, s) in enumerate(sorted_models, 1)]
    
    # if total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)'] < total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)']:
    #     time_advantage = total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)'] - total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)']
    #     print(f"   LSTM –±—ã—Å—Ç—Ä–µ–µ –Ω–∞: {time_advantage:.2f} —Å–µ–∫")
    # else:
    #     time_advantage = total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è LSTM (—Å–µ–∫)'] - total_stats['–û–±—â–µ–µ –≤—Ä–µ–º—è GPT2 (—Å–µ–∫)']
    #     print(f"   GPT2 –±—ã—Å—Ç—Ä–µ–µ –Ω–∞: {time_advantage:.2f} —Å–µ–∫")
    
    
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
    stats_df = pd.DataFrame([total_stats])
    stats_path = current_dir / 'results' / f'model_stats_controlled_length_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
    stats_df.to_csv(stats_path, index=False)
    
    return results_df, total_stats, best_model


# –ó–∞–ø—É—Å–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
if __name__ == "__main__":
    results, stats, best_model = compare_models_lstm_vs_gpt2()