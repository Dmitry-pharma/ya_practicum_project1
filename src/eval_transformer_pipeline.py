from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
from tqdm import tqdm

def evaluate_transformer(model_name="distilgpt2", test_texts=None):
    """
    –û—Ü–µ–Ω–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å LSTM
    """
    print(f"üß™ –û—Ü–µ–Ω–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –º–æ–¥–µ–ª–∏: {model_name}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ pipeline –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
    text_generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=0 if torch.cuda.is_available() else -1
    )
    
    # –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    if test_texts is None:
        test_texts = [
            "I love to",
            "The weather is",
            "I think that",
            "In my opinion",
            "The best way to"
        ]
    
    print("\nüîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö:")
    print("=" * 50)
    
    for i, prompt in enumerate(test_texts, 1):
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
            result = text_generator(
                prompt,
                max_length=len(prompt.split()) + 5,
                num_return_sequences=1,
                pad_token_id=tokenizer.eos_token_id
            )
            
            generated_text = result[0]['generated_text']
            continuation = generated_text[len(prompt):].strip()
            
            print(f"{i}. –ü—Ä–æ–º–ø—Ç: '{prompt}'")
            print(f"   –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ: '{continuation}'")
            print()
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è '{prompt}': {e}")
    
    # –û—Ü–µ–Ω–∫–∞ perplexity (–µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ)
    if test_texts:
        try:
            perplexity = calculate_perplexity(model, tokenizer, test_texts, device)
            print(f"üìä Perplexity –º–æ–¥–µ–ª–∏: {perplexity:.2f}")
        except:
            print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å perplexity")
    
    return model, tokenizer

def calculate_perplexity(model, tokenizer, texts, device, max_length=512):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ perplexity –Ω–∞ –Ω–∞–±–æ—Ä–µ —Ç–µ–∫—Å—Ç–æ–≤
    """
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for text in tqdm(texts, desc="Calculating perplexity"):
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=max_length)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            outputs = model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss
            
            total_loss += loss.item() * inputs["input_ids"].size(1)
            total_tokens += inputs["input_ids"].size(1)
    
    avg_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(avg_loss))
    
    return perplexity.item()

if __name__ == "__main__":
    evaluate_transformer("distilgpt2")