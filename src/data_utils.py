from tqdm import tqdm  
import pandas as pd
import re 
from pathlib import Path
import html
from transformers import AutoTokenizer
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
import pickle

from src.next_token_dataset import TweetsDataset

def clean_text(text):
    text = text.lower()  # –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É  
    text = html.unescape(text)# –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç &quot; -> ", &amp; -> &, –∏ —Ç.–¥.# 2. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å HTML —Å–∏–º–≤–æ–ª—ã 
    text = re.sub(r'<[^>]+>', '', text)# 3. –£–¥–∞–ª–∏—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è HTML —Ç–µ–≥–∏
    # text = re.sub(r'@\w+', 'user', text)# –∑–∞–º–µ–Ω–∏—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è (@username)  
    text = re.sub(r'@\w+', '', text)# –∑–∞–º–µ–Ω–∏—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è (@username)  
    
    
    text = re.sub(r'#\w+', '', text)# –£–¥–∞–ª–∏—Ç—å —Ö–µ—à—Ç–µ–≥–∏ (#tag)  
    text = re.sub(r'[^\w\s,\.!?;:]', '', text)# –£–¥–∞–ª–∏—Ç—å —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)# –£–¥–∞–ª–∏—Ç—å —Å—Å—ã–ª–∫–∏ 
    text = re.sub(r"[^a-z0-9 ]+", " ", text)# –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã
    text = re.sub(r"\s+", " ", text).strip()# —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã
    return text

def load_and_clean_data(file_path, limit=10000):
    with open(file_path, 'r', encoding='utf-8') as f:
        texts = f.readlines()
    
    texts = [text.strip() for text in texts]
    texts = texts[:limit]
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Å—Ç—Ä–æ–∫")
    
    print("Cleaning...")
    cleaned_texts = []
    for text in tqdm(texts):
        cleaned_texts.append(clean_text(text))
    #–¥–ª—è —Ü–µ–ª–µ–π –æ—Ç–ª–∞–¥–∫–∏
    text_pairs = [(i, texts[i], cleaned_texts[i]) for i in range(len(texts))]
    texts_df = pd.DataFrame(text_pairs, columns=['rowno','text_raw','text_cleaned'])
    
    print("Cleaning READY!")
    return texts_df


def prepare_training_pairs3(texts_df, tokenizer, MAX_LEN=20):
    print("Preparing X, Y pairs...")
    data = []
    
    # for rowno, text in tqdm(zip(texts_df['rowno'], texts_df['text_cleaned']), total=len(texts_df)):
    for text in tqdm( texts_df['text_cleaned']):
    
        # print(rowno, text)
        #–µ—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ —Ç–µ–∫—Å—Ç, —Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∫—É
        if text is None or isinstance(text, float):
            continue
        
        tokens = tokenizer.tokenize(text)
        
        if len(tokens) < 2:
            continue
        
        
        for i in range(0, len(tokens) - MAX_LEN, MAX_LEN // 2):  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ 50%
            chunk = tokens[i:i + MAX_LEN + 1]  # +1 –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
            
            if len(chunk) < 2:
                continue
                
        
            start_idx = max(0, i - MAX_LEN)
            x_tok = chunk[start_idx:i]
            y_tok = chunk[start_idx+1:i+1]
            
            if len(x_tok) < 1 or len(y_tok) < 1:
                continue
            
            # print(x_tok,y_tok)    
            
            x_ids = tokenizer.convert_tokens_to_ids(x_tok)
            y_ids = tokenizer.convert_tokens_to_ids(y_tok)
            
            x_padded = [tokenizer.pad_token_id] * (MAX_LEN - len(x_ids)) + x_ids
            y_padded = [tokenizer.pad_token_id] * (MAX_LEN - len(y_ids)) + y_ids
            attention_mask = [1 if token_id != tokenizer.pad_token_id else 0 for token_id in x_padded]
            
            data.append((x_padded, y_padded, attention_mask))
    
    print("PAIRS ARE READY!")
    return data




# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
def save_file(dataset, file_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–∞–π–ª"""
    # with open(file_path, 'wb') as f:
    #     pickle.dump(dataset, f)
    # print(f"üíæ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {file_path}")
    # –°–æ–∑–¥–∞–µ–º CSV —Ñ–∞–π–ª —Å —Ç–µ–º –∂–µ –∏–º–µ–Ω–µ–º –Ω–æ –¥—Ä—É–≥–∏–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º
    # csv_path = file_path.with_suffix('.csv')
    
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç –≤ DataFrame –¥–ª—è CSV
        if hasattr(dataset, '__getitem__') and hasattr(dataset, '__len__'):
            # –î–ª—è TweetsDataset
            data_for_csv = []
            for i in range(min(1000, len(dataset))):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
                try:
                    item = dataset[i]
                    row = {
                        'index': i,
                        'input_ids': item['data'].tolist(),
                        'target_ids': item['target'].tolist(),
                        'attention_mask': item['mask'].tolist()
                    }
                    data_for_csv.append(row)
                except Exception as e:
                    continue
            
            df = pd.DataFrame(data_for_csv)
            df.to_csv(file_path, index=False, encoding='utf-8')
            print(f"üíæ –ü—Ä–∏–º–µ—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ CSV: {file_path} ({len(df)} —Å—Ç—Ä–æ–∫)")
            
        else:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å CSV –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ç–∏–ø–∞ {type(dataset)}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ CSV: {e}")

def save_samples(texts_df, file_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –≤ CSV —Ñ–∞–π–ª"""
    texts_df[['rowno', 'text_raw', 'text_cleaned']].to_csv(file_path, index=False, encoding='utf-8')
    print(f"–¢–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {file_path}")

def load_samples(file_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –≤ CSV —Ñ–∞–π–ª"""
    # texts_df[['rowno', 'text_raw', 'text_cleaned']].to_csv(file_path, index=False, encoding='utf-8')
    texts_df= pd.read_csv(file_path, encoding='utf-8')
    print(f"–¢–∞–±–ª–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {file_path}")
    return texts_df

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
def load_datasets(file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç—ã –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    
    with open(file_path, 'rb') as f:
        v_dataset = pickle.load(f)
        
    # print(f"üìÇ –î–∞—Ç–∞—Å–µ—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑: {config['datasets_dir']}")
    return v_dataset

# –ü—Ä–∏–º–µ—Ä –∑–∞–≥—Ä—É–∑–∫–∏ (–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö —Å–∫—Ä–∏–ø—Ç–∞—Ö)
# train_ds_loaded, val_ds_loaded, test_ds_loaded = load_datasets(config)    
# samples_preparation(file_path=config['file_path'],limit=config['limit'],tokenizer=tokenizer,MAX_LEN=config['MAX_LEN'],batch_size=config['batch_size'])
def samples_preparation(data_dir, source_file, limit):
       
    file_path = data_dir / source_file
    texts_df = load_and_clean_data(file_path, limit)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    train, temp = train_test_split(texts_df, test_size=0.2, random_state=42)
    val, test = train_test_split(temp, test_size=1/2, random_state=42)
    
    # print(test)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –≤—ã–±–æ—Ä–∫–∏
    save_samples(train, data_dir / 'train_dataset.csv')
    save_samples(val, data_dir / 'val_dataset.csv')
    save_samples(test, data_dir / 'test_dataset.csv')
    return train,val,test
    
def dataset_preparation(data, tokenizer, MAX_LEN=20, batch_size=128, shuffle=True):         
    X, Y, M = zip(*prepare_training_pairs3(data, tokenizer, MAX_LEN))
   
    data_ds = TweetsDataset(X, Y, M)
    data_loader = DataLoader(data_ds, batch_size=batch_size, shuffle=shuffle)
    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: Size={len(data_ds)}")
    return data_ds, data_loader

# --------------------------------------------
# import os
# from torch.utils.data import Dataset
# import torch

# class TweetsDataset(Dataset):
#     def __init__(self, data, targets, mask):
#         self.data = data
#         self.targets = targets 
#         self.mask = mask

#     def __len__(self):
#         return len(self.data)

#     def __getitem__(self, idx):      
#         return {
#             'data': torch.tensor(self.data[idx], dtype=torch.long),
#             'target': torch.tensor(self.targets[idx], dtype=torch.long),
#             'mask': torch.tensor(self.mask[idx], dtype=torch.long)
#         }
        

# current_dir = Path(os.getcwd())
# v_file_path=Path(current_dir) /'data'/ 'test_dataset.csv'
# print(v_file_path)
# #—Ñ–æ—Ä–º–∏—Ä—É–µ–º/–∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
# test= load_samples(v_file_path )        
# transformer_model_name = "distilgpt2"

# tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)
# test_ds,  test_loader = dataset_preparation(test, tokenizer, MAX_LEN=20, batch_size=10)

# for i in range(len(test_ds)):
#         print(test_ds[i])


