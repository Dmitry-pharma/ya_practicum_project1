# https://code.s3.yandex.net/deep-learning/tweets.txt
import torch.nn as nn
from sklearn.metrics import accuracy_score
import re
import numpy as np
import pandas as pd
from datasets import load_dataset
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from transformers import AutoTokenizer
from pathlib import Path
import html
from torch.utils.data import Dataset, DataLoader
#---------------------------------------------------------------------------
file_path = Path(__file__).parent.parent / 'data' / '1_raw_dataset_tweets.txt'
print(file_path)

# v_path="../data/raw_dataset_tweets.txt"
# v_path="C:/Users/OMEN/Documents/LLM_Test/YaPracticum/project1_text-autocomplete/data/raw_dataset_tweets.txt"
# texts_df = pd.read_csv(file_path)
# print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(dtexts_df)}")

# –ü—Ä–æ—Å—Ç–æ —á–∏—Ç–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏
with open(file_path, 'r', encoding='utf-8') as f:
    texts = f.readlines()

# –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç—Ä–æ–∫
texts = [text.strip() for text in texts]
texts = texts[:10]
print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Å—Ç—Ä–æ–∫")
# for i, text in enumerate(texts[:5]):  # –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫
#     print(f"{i+1}: {text}")
    
def clean_text(text):
    text = text.lower()  # –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É    
    text = html.unescape(text)  # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç &quot; -> ", &amp; -> &, –∏ —Ç.–¥.# 2. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å HTML —Å–∏–º–≤–æ–ª—ã  
    text = re.sub(r'<[^>]+>', '', text)  # 3. –£–¥–∞–ª–∏—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è HTML —Ç–µ–≥–∏
    text = re.sub(r'@\w+', 'user', text)# –∑–∞–º–µ–Ω–∏—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è (@username)    
    text = re.sub(r'#\w+', '', text) # –£–¥–∞–ª–∏—Ç—å —Ö–µ—à—Ç–µ–≥–∏ (#tag)    
    text = re.sub(r'[^\w\s,\.!?;:]', '', text) # –£–¥–∞–ª–∏—Ç—å —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)# –£–¥–∞–ª–∏—Ç—å —Å—Å—ã–ª–∫–∏    
   
    text = re.sub(r"[^a-z0-9 ]+", " ", text)  # –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã
    text = re.sub(r"\s+", " ", text).strip()  # —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã
    
    return text

print("Cleaning...")
cleaned_texts =[]
for text in tqdm(texts):
    cleaned_texts.append(clean_text(text))
    
text_pairs = [(i, texts[i], cleaned_texts[i]) for i in range(len(texts))]

#—Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ csv
texts_df = pd.DataFrame(text_pairs, columns=['rowno','text_raw','text_cleaned'])
# output_path = Path(__file__).parent.parent / 'data' / '2_cleaned_dataset_tweets.csv'
# texts_df.to_csv(output_path, index=False, encoding='utf-8')



# from transformers import BertTokenizerFast
# import torch
from torch.utils.data import Dataset, DataLoader

# tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
class TweetRNNDataset(Dataset):
    def __init__(self, texts, tokenizer, max_len=20):
        # self.encodings = tokenizer(texts, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')
        # self.labels = torch.tensor(labels)#, dtype=torch.long)
        # self.samples = []
        # self.tokenizer = tokenizer
        # self.max_len = max_len
        
        print("Preparing X, Y pairs...")
        for text in tqdm(texts):
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            tokens = tokenizer.tokenize(text)
            
            if len(tokens) < 2:  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
                continue
                
            # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞ —Ä–∞–∑–º–µ—Ä–æ–º max_len
            for i in range(1, len(tokens)):
                start_idx = max(0, i - max_len)
                x_tok = tokens[start_idx:i]  # —Ç–æ–∫–µ–Ω—ã —Å start_idx –¥–æ i-1        
                y_tok = tokens[start_idx+1:i+1]  # –¶–µ–ª—å: —Ç–µ –∂–µ —Ç–æ–∫–µ–Ω—ã, –Ω–æ —Å–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞ 1 –≤–ø–µ—Ä–µ–¥
                
                if len(x_tok) < 1 or len(y_tok) < 1:
                    continue
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–∫–µ–Ω—ã –≤ ID
                x_ids = tokenizer.convert_tokens_to_ids(x_tok)
                y_ids = tokenizer.convert_tokens_to_ids(y_tok)
                
                # –ü–∞–¥–¥–∏–Ω–≥ —Å–ª–µ–≤–∞ –¥–ª—è X/Y –¥–æ max_len
                x_padded = [tokenizer.pad_token_id] * (max_len - len(x_ids)) + x_ids
                y_padded = [tokenizer.pad_token_id] * (max_len - len(y_ids)) + y_ids
                
                # –°–æ–∑–¥–∞–µ–º attention_mask (1 –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, 0 –¥–ª—è –ø–∞–¥–¥–∏–Ω–≥–∞)
                attention_mask = [1 if token_id != tokenizer.pad_token_id else 0 for token_id in x_padded]
                
                 # –î–ª—è —Ç–æ–∫–µ–Ω–æ–≤ —Ç–æ–∂–µ –¥–µ–ª–∞–µ–º –ø–∞–¥–¥–∏–Ω–≥, —á—Ç–æ–±—ã –æ–Ω–∏ –±—ã–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã
                x_tok_padded = ['[PAD]'] * (max_len - len(x_tok)) + x_tok
                y_tok_padded = ['[PAD]'] * (max_len - len(y_tok)) + y_tok
                
                # print(i,x_tok,y_tok,x_padded,attention_mask,y_padded)
                self.samples.append({
                    'x':x_tok_padded,
                    'y':y_tok_padded,
                    'input_ids': x_padded,
                    'attention_mask': attention_mask,
                    'labels': y_padded
                })
        
        print(f"PAIRS ARE READY! Prepeared {len(self.samples)} pairs")
        
    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        return {
            'input_ids': sample['input_ids'],
            'attention_mask': sample['attention_mask'],
            'labels': sample['labels'],
            'x':sample['x'],
            'y':sample['y']
        }

# X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)
train, temp = train_test_split(texts_df['text_cleaned'].head(1000), test_size=0.2, random_state=42)
val, test = train_test_split(temp, test_size=1/2, random_state=42)  # val 0.1, test 0.2

train_ds = TweetRNNDataset(train, tokenizer)
val_ds = TweetRNNDataset(val, tokenizer)
test_ds = TweetRNNDataset(test, tokenizer)

train_loader = DataLoader(train_ds, batch_size=128)#, shuffle=True)
val_loader = DataLoader(val_ds)#, batch_size=128)
test_loader = DataLoader(test_ds)#, batch_size=128)

print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç—ã —Å–æ–∑–¥–∞–Ω—ã: Train={len(train_ds)}, Val={len(val_ds)}, Test={len(test_ds)}")

# # –î–û–ë–ê–í–õ–ï–ù –ö–û–î –î–õ–Ø –í–´–í–û–î–ê 10 –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò–ó DATALOADER
# print("\n" + "="*60)
# print("–í–´–í–û–î 10 –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò–ó DATALOADER")
# print("="*60)
n_epochs = 3

for epoch in range(n_epochs): 
    for one in (train_loader):
        print(one['input_ids'])
        
# for batch_idx, batch in enumerate(train_loader):
#     if batch_idx >= 10:
#         break
#     print(batch_idx, batch)
    
# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è ID –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç
# def decode_batch(batch, tokenizer):
#     """–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –±–∞—Ç—á –æ–±—Ä–∞—Ç–Ω–æ –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç"""
#     decoded_samples = []
#     for i in range(len(batch['input_ids'])):
#         # –î–µ–∫–æ–¥–∏—Ä—É–µ–º input_ids (–∫–æ–Ω—Ç–µ–∫—Å—Ç)
#         input_ids = batch['input_ids'][i]
#         input_tokens = tokenizer.convert_ids_to_tokens(input_ids)
#         input_text = tokenizer.convert_tokens_to_string([t for t in input_tokens if t != tokenizer.pad_token])
        
#         # –î–µ–∫–æ–¥–∏—Ä—É–µ–º labels (—Ü–µ–ª–µ–≤–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
#         label_ids = batch['labels'][i]
#         label_tokens = tokenizer.convert_ids_to_tokens(label_ids)
#         label_text = tokenizer.convert_tokens_to_string([t for t in label_tokens if t != tokenizer.pad_token])
        
#         # Attention mask
#         attention_mask = batch['attention_mask'][i]
        
#         decoded_samples.append({
#             'x':batch['x'][i],
#             'y':batch['y'][i],
#             'input_text': input_text,
#             'label_text': label_text,
#             'input_ids': input_ids.tolist(),
#             'label_ids': label_ids.tolist(),
#             'attention_mask': attention_mask.tolist()
#         })
    
#     return decoded_samples

# # –í—ã–≤–æ–¥–∏–º 10 –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ train_loader
# print("\nüéØ 10 –ü–†–ò–ú–ï–†–û–í –ò–ó TRAIN DATALOADER:")
# print("-" * 60)

# batch_count = 0
# total_examples = 0

# for batch_idx, batch in enumerate(train_loader):
#     decoded_batch = decode_batch(batch, tokenizer)
    
#     for example_idx, example in enumerate(decoded_batch):
#         if total_examples >= 10:
#             break
            
#         print(f"\nüìù –ü—Ä–∏–º–µ—Ä {total_examples + 1}:")
#         print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç (X): '{example['input_text']}'")
#         print(f"   –¶–µ–ª—å (Y):     '{example['label_text']}'")
#         print(f"   Input IDs:    {example['input_ids']}")
#         print(f"   Label IDs:    {example['label_ids']}")
#         print(f"   Attn Mask:    {example['attention_mask']}")
#         print(f"   X:    {example['x']}")
#         print(f"   Y:    {example['y']}")
        
        
#         total_examples += 1
    
#     if total_examples >= 10:
#         break
    
#     batch_count += 1

# print(f"\nüìä –ü—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ –±–∞—Ç—á–µ–π: {batch_count + 1}")
# print(f"üìä –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch['input_ids'].shape[1]}")

# # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ç–µ–Ω–∑–æ—Ä–∞–º
# print("\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–ï–ù–ó–û–†–û–í:")
# print(f"   Input IDs shape: {batch['input_ids'].shape}")
# print(f"   Labels shape: {batch['labels'].shape}")
# print(f"   Attention Mask shape: {batch['attention_mask'].shape}")
# print(f"   Input IDs dtype: {batch['input_ids'].dtype}")
# print(f"   Labels dtype: {batch['labels'].dtype}")

# # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø–∞–¥–¥–∏–Ω–≥–∞
# print(f"\nüîç –ü–†–û–í–ï–†–ö–ê –ü–ê–î–î–ò–ù–ì–ê:")
# input_ids = batch['input_ids'][0]
# pad_count = (input_ids == tokenizer.pad_token_id).sum().item()
# real_token_count = len(input_ids) - pad_count
# print(f"   –¢–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–∏–º–µ—Ä–µ: {len(input_ids)}")
# print(f"   –†–µ–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {real_token_count}")
# print(f"   –ü–∞–¥–¥–∏–Ω–≥ —Ç–æ–∫–µ–Ω–æ–≤: {pad_count}")
# print(f"   –ü–∞–¥–¥–∏–Ω–≥ —Ç–æ–∫–µ–Ω ID: {tokenizer.pad_token_id}")

# # –í—ã–≤–æ–¥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ validation loader –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
# print("\nüîç –î–õ–Ø –°–†–ê–í–ù–ï–ù–ò–Ø - 2 –ü–†–ò–ú–ï–†–ê –ò–ó VALIDATION DATALOADER:")
# print("-" * 60)

# val_examples_shown = 0
# for batch_idx, batch in enumerate(val_loader):
#     decoded_batch = decode_batch(batch, tokenizer)
    
#     for example_idx, example in enumerate(decoded_batch[:2]):  # —Ç–æ–ª—å–∫–æ 2 –ø—Ä–∏–º–µ—Ä–∞
#         print(f"\nüìù Val –ü—Ä–∏–º–µ—Ä {val_examples_shown + 1}:")
#         print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç: '{example['input_text']}'")
#         print(f"   –¶–µ–ª—å:     '{example['label_text']}'")
        
#         val_examples_shown += 1
#         if val_examples_shown >= 2:
#             break
    
#     if val_examples_shown >= 2:
#         break

# print("\n‚úÖ –í—ã–≤–æ–¥ –ø—Ä–∏–º–µ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω!")

# class MeanPoolingRNN(nn.Module):
#     def __init__(self, vocab_size, emb_dim=300, hidden_dim=256, output_dim=2, pad_idx=0):
#         super().__init__()
#         self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)
#         self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)# RNN-—Å–ª–æ–π
#         self.norm = nn.LayerNorm(hidden_dim) # —Å–ª–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
#         self.dropout = nn.Dropout(0.5)# dropout-—Å–ª–æ–π
#         self.fc = nn.Linear(hidden_dim, output_dim) # –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
#         self.init_weights()


#     def init_weights(self):
#         # –Ω–∞–ø–∏—à–∏—Ç–µ xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤
#         # if isinstance(self, nn.Linear) or isinstance(self, nn.RNN):
#         #     nn.init.xavier_uniform_(self.weight)
#         #     if self.bias is not None:
#         #         nn.init.zeros_(self.bias)
#         # nn.init.xavier_uniform_(self.fc.weight)
#         # for name, param in self.rnn.named_parameters():
#         #     if 'weight' in name:
#         #         nn.init.xavier_uniform_(param)
#         for name, param in self.named_parameters():
#             if 'weight' in name and param.dim() > 1:
#                 nn.init.xavier_uniform_(param)
#             elif 'bias' in name:
#                 nn.init.constant_(param, 0.0)


#     def forward(self, input_ids, attention_mask):
#         x = self.embedding(input_ids)
#         rnn_out, _ = self.rnn(x)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç rnn-—Å–ª–æ—è

#         rnn_out_normed = self.norm(rnn_out)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è rnn_out


#         # mean pooling –ø–æ attention_mask
#         mask = attention_mask.unsqueeze(2).expand_as(rnn_out)# –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –º–∞—Å–∫—É –¥–ª—è rnn_out_normed
#         # mask = mask.unsqueeze(-1).float()
#         masked_out =  rnn_out_normed * mask# —É–º–Ω–æ–∂—å—Ç–µ rnn_out_normed –Ω–∞ –º–∞—Å–∫—É
#         summed = masked_out.sum(dim=1)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Å—É–º–º—É masked_out –ø–æ dim=1
#         lengths = attention_mask.sum(dim=1).unsqueeze(1)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
#         mean_pooled = summed / lengths# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Å—Ä–µ–¥–Ω–∏–µ, —Ä–∞–∑–¥–µ–ª–∏–≤ —Å—É–º–º—ã –Ω–∞ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
#         out = self.dropout(mean_pooled)# –ø—Ä–∏–º–µ–Ω–∏—Ç–µ dropout
#         logits = self.fc(out)# –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π 
#         return logits


# # —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# pad_token_id = tokenizer.pad_token_id  # –¥–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ pad_token_id
# model = MeanPoolingRNN(vocab_size=tokenizer.vocab_size, pad_idx=pad_token_id).to(device)


# # —Å–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)#, weight_decay=3e-3) # —Å–æ–∑–¥–∞–π—Ç–µ –æ–ø—Ç–∏–º–∞–π–∑–µ—Ä —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
# criterion = nn.CrossEntropyLoss()


# # –∫–æ–¥ –æ–±—É—á–µ–Ω–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏
# def train_epoch(model, loader):
#     model.train()
#     total_loss = 0
#     for batch in loader:
#         ids = batch['input_ids'].to(device)
#         mask = batch['attention_mask'].to(device)
#         labels = batch['label'].to(device)



#         optimizer.zero_grad()# –æ–±–Ω—É–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
#         logits = model(ids, mask)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏
        
#         # print(f"Logits shape: {logits.shape}")  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å [128, 2]
#         # print(f"Labels shape: {labels.shape}")  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å [128]
#         # print(f"Labels dtype: {labels.dtype}")  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å torch.int64
        
#         loss = criterion(logits, labels)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å
#         loss.backward()  # –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # –ø—Ä–∏–º–µ–Ω–∏—Ç–µ gradient clipping
#         optimizer.step() # –æ–±–Ω–æ–≤–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

#         total_loss += loss.item()

        
        
#     return total_loss / len(loader)


# # –∫–æ–¥ –ø–æ–¥—Å—á—ë—Ç–∞ accuracy –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
# def evaluate(model, loader):
#     model.eval()
#     preds, trues = [], []
#     with torch.no_grad():
#         for batch in loader:
#             ids = batch['input_ids'].to(device)
#             mask = batch['attention_mask'].to(device)
#             labels = batch['label']
#             logits = model(ids, mask)
#             preds += torch.argmax(logits, dim=1).cpu().tolist()
#             trues += labels.tolist()
#     return accuracy_score(trues, preds)


# # –æ–±—É—á–µ–Ω–∏–µ
# for epoch in range(10):
#     loss = train_epoch(model, train_loader)
#     acc = evaluate(model, val_loader)
#     print(f"Epoch {epoch+1}: Loss = {loss:.4f}, Accuracy = {acc:.4f}")