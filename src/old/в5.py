from tqdm import tqdm  
import pandas as pd
import re 
from collections import Counter
from pathlib import Path
import html
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizerFast
import torch
import json
import random
from datetime import datetime
# tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
from transformers import AutoTokenizer
#-----------parameters---------------------------
file_path = Path(__file__).parent.parent / 'data' / '1_raw_dataset_tweets.txt'
print(file_path)
model_dir = Path('C:/Users/OMEN/Documents/LLM_Test/YaPracticum/project1_text-autocomplete/models')  # –∏–ª–∏ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π –ø—É—Ç—å
model_dir.mkdir(exist_ok=True)
MAX_LEN = 20  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–∏–º–µ—Ä–∞ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
limit=1000 #–∫–æ–ª-–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ –≤—Ö–æ–¥–µ (–¥–æ —Ä–∞–∑–∏–µ–Ω–∏—è –Ω–∞ –≤—ã–±–æ—Ä–∫–∏)
v_hidden_dim=512 #—Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
v_emb_dim=300 #—Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
v_batch_size=128
rnn_type="LSTM"  #"GRU"  "LSTM"  "RNN"
# v_path="../data/raw_dataset_tweets.txt"
# v_path="C:/Users/OMEN/Documents/LLM_Test/YaPracticum/project1_text-autocomplete/data/raw_dataset_tweets.txt"
# texts_df = pd.read_csv(file_path)
# print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(dtexts_df)}")
#-------------------------------------------------------------------
# –ü—Ä–æ—Å—Ç–æ —á–∏—Ç–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏
with open(file_path, 'r', encoding='utf-8') as f:
    texts = f.readlines()

# –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç—Ä–æ–∫
texts = [text.strip() for text in texts]
texts = texts[:limit]

print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Å—Ç—Ä–æ–∫")
# for i, text in enumerate(texts[:5]):  # –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫
#     print(f"{i+1}: {text}")
    
def clean_text(text):
    text = text.lower()  # –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É    
    text = html.unescape(text)  # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç &quot; -> ", &amp; -> &, –∏ —Ç.–¥.# 2. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å HTML —Å–∏–º–≤–æ–ª—ã  
    text = re.sub(r'<[^>]+>', '', text)  # 3. –£–¥–∞–ª–∏—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è HTML —Ç–µ–≥–∏
    text = re.sub(r'@\w+', 'user', text)# –∑–∞–º–µ–Ω–∏—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è (@username)    
    text = re.sub(r'#\w+', '', text) # –£–¥–∞–ª–∏—Ç—å —Ö–µ—à—Ç–µ–≥–∏ (#tag)    
    text = re.sub(r'[^\w\s,\.!?;:]', '', text) # –£–¥–∞–ª–∏—Ç—å —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)# –£–¥–∞–ª–∏—Ç—å —Å—Å—ã–ª–∫–∏    
   
    text = re.sub(r"[^a-z0-9 ]+", " ", text)  # –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã
    text = re.sub(r"\s+", " ", text).strip()  # —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã
    
    return text
print("Cleaning...")
cleaned_texts =[]
for text in tqdm(texts):
    cleaned_texts.append(clean_text(text))
    
text_pairs = [(i, texts[i], cleaned_texts[i]) for i in range(len(texts))]

#—Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ csv
texts_df = pd.DataFrame(text_pairs, columns=['rowno','text_raw','text_cleaned'])
# output_path = Path(__file__).parent.parent / 'data' / '2_cleaned_dataset_tweets.csv'
# texts_df.to_csv(output_path, index=False, encoding='utf-8')

print("Cleanning READY!")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")


print("Prepearing X, Y pairs...")
data = []
for text in tqdm(texts_df['text_cleaned']):  # df['clean'] ‚Äî –æ—á–∏—â–µ–Ω–Ω—ã–µ —Ç–≤–∏—Ç—ã
    tokens = tokenizer.tokenize(text)
    if len(tokens) < 2:# –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
        continue
    # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–≤–∏—Ç–æ–≤ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞ —Ä–∞–∑–º–µ—Ä–æ–º MAX_LEN
    for i in range(1, len(tokens)):
        start_idx = max(0, i - MAX_LEN)
        x_tok = tokens[start_idx:i]# —Ç–æ–∫–µ–Ω—ã —Å start_idx –¥–æ i-1        
        y_tok = tokens[start_idx+1:i+1]# –¶–µ–ª—å: —Ç–µ –∂–µ —Ç–æ–∫–µ–Ω—ã, –Ω–æ —Å–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞ 1 –≤–ø–µ—Ä–µ–¥ (—Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω)
        if len(x_tok) < 1 or len(y_tok) < 1:
            continue
        # padding —Å–ª–µ–≤–∞ –¥–ª—è X/Y –¥–æ MAX_LEN
        # x_ids = [0] * (MAX_LEN-len(x_tok)) + [vocab.get(tok, 1) for tok in x_tok]
        # y_ids = [0] * (MAX_LEN-len(y_tok)) + [vocab.get(tok, 1) for tok in y_tok]
        # data.append((x_ids, y_ids))
        # data.append((x_tok,y_tok))
         # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–∫–µ–Ω—ã –≤ ID
        x_ids = tokenizer.convert_tokens_to_ids(x_tok)
        y_ids = tokenizer.convert_tokens_to_ids(y_tok)                
        # –ü–∞–¥–¥–∏–Ω–≥ —Å–ª–µ–≤–∞ –¥–ª—è X/Y –¥–æ max_len
        x_padded = [tokenizer.pad_token_id] * (MAX_LEN - len(x_ids)) + x_ids
        y_padded = [tokenizer.pad_token_id] * (MAX_LEN - len(y_ids)) + y_ids                
        # –°–æ–∑–¥–∞–µ–º attention_mask (1 –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, 0 –¥–ª—è –ø–∞–¥–¥–∏–Ω–≥–∞)
        attention_mask = [1 if token_id != tokenizer.pad_token_id else 0 for token_id in x_padded]
        # print(i,x_tok,y_tok,x_padded, y_padded,attention_mask)
        # print()
        data.append((x_padded, y_padded,attention_mask))
print("PAIRS ARE READY!")


class TweetsDataset(Dataset):
    def __init__(self, data, targets,mask):
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ data –∏ targets –≤ –∞—Ç—Ä–∏–±—É—Ç—ã
        # –í–∞—à –∫–æ–¥ –∑–¥–µ—Å—å
        self.data = data
        self.targets = targets 
        self.mask=mask

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):

        return {'data':torch.tensor(self.data[idx], dtype=torch.long),
                'target':torch.tensor(self.targets[idx], dtype=torch.long),
                'mask':torch.tensor(self.mask[idx], dtype=torch.long)                
            }

from sklearn.model_selection import train_test_split 
train, temp = train_test_split(data, test_size=0.2, random_state=42)
val, test = train_test_split(temp, test_size=1/2, random_state=42)  # val 0.1, test 0.2

#—Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–æ—Ä—Ç–µ–∂–µ–π —á–µ—Ä–µ–∑ zip
X_train, Y_train, M_train = zip(*train)
X_val, Y_val, M_val = zip(*val)
X_test, Y_test, M_test = zip(*test)

train_ds = TweetsDataset(X_train, Y_train, M_train)
val_ds = TweetsDataset(X_val, Y_val, M_val)
test_ds = TweetsDataset(X_test, Y_test, M_test)

train_loader = DataLoader(train_ds, batch_size=v_batch_size, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=v_batch_size)
test_loader = DataLoader(test_ds, batch_size=v_batch_size)

print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç—ã —Å–æ–∑–¥–∞–Ω—ã: Train={len(train_ds)}, Val={len(val_ds)}, Test={len(test_ds)}")

# for batch_idx, batch in enumerate(train_loader):
#     if batch_idx >= 1:
#         break
#     print("BATCH Number=",batch_idx)
#     for batch_item_num, batch_item in enumerate(batch):
#         print(batch_item_num)
#         print("x=",batch['data'][batch_item_num])
#         print("y=",batch['target'][batch_item_num])
#         print("m=",batch['mask'][batch_item_num])
 
# –ü—Ä–æ–≤–µ—Ä–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
# print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π:")
# sample_batch = next(iter(train_loader))
# print(f"inputs shape: {sample_batch['data'].shape}")  # [128, 20]
# print(f"labels shape: {sample_batch['target'].shape}")        # [128, 20] - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å!
# print(f"attention_mask shape: {sample_batch['mask'].shape}")  # [128, 20]


import torch.nn as nn
from sklearn.metrics import accuracy_score

class NextPhrasePredictionRNN(nn.Module):
    def __init__(self, rnn_type="RNN", vocab_size=30522, emb_dim=300, hidden_dim=256, output_dim=2, pad_idx=0):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)
        
        rnn_cls = {"RNN": nn.RNN, "GRU": nn.GRU, "LSTM": nn.LSTM}[rnn_type]
        self.rnn = rnn_cls(emb_dim, hidden_dim, batch_first=True) 

        # self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)# RNN-—Å–ª–æ–π
        self.norm = nn.LayerNorm(hidden_dim) # —Å–ª–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.dropout = nn.Dropout(0.5)# dropout-—Å–ª–æ–π
        self.fc = nn.Linear(hidden_dim, vocab_size) # –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self.init_weights()


    def init_weights(self):
        # –Ω–∞–ø–∏—à–∏—Ç–µ xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤
        # if isinstance(self, nn.Linear) or isinstance(self, nn.RNN):
        #     nn.init.xavier_uniform_(self.weight)
        #     if self.bias is not None:
        #         nn.init.zeros_(self.bias)
        # nn.init.xavier_uniform_(self.fc.weight)
        # for name, param in self.rnn.named_parameters():
        #     if 'weight' in name:
        #         nn.init.xavier_uniform_(param)
        for name, param in self.named_parameters():
            if 'weight' in name and param.dim() > 1:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0.0)


    def forward(self, input_ids, attention_mask):
        x = self.embedding(input_ids)
        rnn_out, _ = self.rnn(x)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç rnn-—Å–ª–æ—è
        
        rnn_out_normed = self.norm(rnn_out)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è rnn_out


        # mean pooling –ø–æ attention_mask
        # mask = attention_mask.unsqueeze(2).expand_as(rnn_out)# –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –º–∞—Å–∫—É –¥–ª—è rnn_out_normed
        mask = attention_mask.unsqueeze(-1).float()  # [batch_size, seq_len, 1]
        # mask = mask.unsqueeze(-1).float()
        masked_out =  rnn_out_normed * mask# —É–º–Ω–æ–∂—å—Ç–µ rnn_out_normed –Ω–∞ –º–∞—Å–∫—É
        summed = masked_out.sum(dim=1)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Å—É–º–º—É masked_out –ø–æ dim=1
        # lengths = attention_mask.sum(dim=1).unsqueeze(1)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        lengths = attention_mask.sum(dim=1).unsqueeze(1).clamp(min=1e-9)  # [batch_size, 1]
        mean_pooled = summed / lengths# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Å—Ä–µ–¥–Ω–∏–µ, —Ä–∞–∑–¥–µ–ª–∏–≤ —Å—É–º–º—ã –Ω–∞ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
         # 5. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        # –ü–æ–≤—Ç–æ—Ä—è–µ–º –æ–±—â–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        seq_representation = mean_pooled.unsqueeze(1).repeat(1, input_ids.size(1), 1)  # [batch_size, seq_len, hidden_dim]
        
        # out = self.dropout(mean_pooled)# –ø—Ä–∏–º–µ–Ω–∏—Ç–µ dropout
        out = self.dropout(seq_representation)
        
        logits = self.fc(out)# –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π 
        return logits


# —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
pad_token_id = tokenizer.pad_token_id  # –¥–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ pad_token_id
# model = MeanPoolingRNN(vocab_size=tokenizer.vocab_size, pad_idx=pad_token_id).to(device)
model = NextPhrasePredictionRNN(
    rnn_type="LSTM",
    vocab_size=tokenizer.vocab_size,
    emb_dim=v_emb_dim,
    hidden_dim=v_hidden_dim,
    pad_idx=pad_token_id
).to(device)

# —Å–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)#, weight_decay=3e-3) # —Å–æ–∑–¥–∞–π—Ç–µ –æ–ø—Ç–∏–º–∞–π–∑–µ—Ä —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)


# –∫–æ–¥ –æ–±—É—á–µ–Ω–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏
def train_epoch(model, loader):
    model.train()
    total_loss = 0
    progress_bar = tqdm(loader, desc="Train", leave=False)
    for batch in progress_bar:
        ids = batch['data'].to(device)
        mask = batch['mask'].to(device)
        labels = batch['target'].to(device)



        optimizer.zero_grad()# –æ–±–Ω—É–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        logits = model(ids, mask)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏
        
        # print(f"Logits shape: {logits.shape}")  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å [128, 2]
        # print(f"Labels shape: {labels.shape}")  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å [128]
        # print(f"Labels dtype: {labels.dtype}")  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å torch.int64
        logits_flat = logits.reshape(-1, logits.size(-1))
        labels_flat = labels.reshape(-1)
        
        # loss = criterion(logits, labels)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å
        loss = criterion(logits_flat, labels_flat)
        loss.backward()  # –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # –ø—Ä–∏–º–µ–Ω–∏—Ç–µ gradient clipping
        optimizer.step() # –æ–±–Ω–æ–≤–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

        total_loss += loss.item()
        # –û–±–Ω–æ–≤–ª—è–µ–º postfix tqdm: –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π loss
        progress_bar.set_postfix({"Loss": f"{loss.item():.4f}"})

        
        
    return total_loss / len(loader)


# –∫–æ–¥ –ø–æ–¥—Å—á—ë—Ç–∞ accuracy –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
def evaluate_xxx(model, loader):
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for batch in loader:
            ids = batch['data'].to(device)
            mask = batch['mask'].to(device)
            labels = batch['target']
            logits = model(ids, mask)
            # preds += torch.argmax(logits, dim=1).cpu().tolist()
            # trues += labels.tolist()
            preds += torch.argmax(logits, dim=-1).cpu().flatten().tolist()
            trues += labels.flatten().tolist()
    return accuracy_score(trues, preds)
# —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–º–µ—Ä–∞ –ª–æ—Å—Å–∞ –∏ accuracy

def evaluate(model, loader):
    model.eval()
    preds, trues = [], []
    # correct, total = 0, 0
    # sum_loss = 0
    total_loss=0
    with torch.no_grad():
        for batch in loader:
            ids = batch['data'].to(device)
            mask = batch['mask'].to(device)
            labels = batch['target']
            logits = model(ids,mask)# –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—Ö–æ–¥–∞ ids
            logits_flat = logits.reshape(-1, logits.size(-1))
            labels_flat = labels.reshape(-1)
            loss = criterion(logits_flat,labels_flat)# —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
            total_loss += loss.item()
            # preds = torch.argmax(x_output, dim=1)# –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            preds += torch.argmax(logits, dim=-1).cpu().flatten().tolist()
            trues += labels.cpu().flatten().tolist()
            # correct += (preds == y_batch).sum().item()# –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ—Ä–Ω–æ —É–≥–∞–¥–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            # total += y_batch.size(0)# —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            # sum_loss += loss.item()# —Å—É–º–º–∞—Ä–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
    
    # –ª–æ—Å—Å –∏ accuracy
    accuracy = accuracy_score(trues, preds)
    avg_loss = total_loss / len(loader)
    # avg_loss = sum_loss / len(loader)
    # accuracy = correct / total
    return accuracy, avg_loss 

def save_model(model, optimizer, epoch, accuracy, loss, path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ –µ—ë –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'accuracy': accuracy,
        'loss': loss,
        'model_config': {
            'vocab_size': tokenizer.vocab_size,
            'emb_dim': v_emb_dim,
            'hidden_dim': v_hidden_dim,
            'pad_idx': pad_token_id
        }
    }, path)

def load_model(model, optimizer, path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –µ—ë –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"""
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    return checkpoint['epoch'], checkpoint['accuracy'], checkpoint['loss']

def test_model(model, loader):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    model.eval()
    all_preds = []
    all_trues = []
    test_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Testing"):
            ids = batch['data'].to(device)
            mask = batch['mask'].to(device)
            labels = batch['target'].to(device)
            
            logits = model(ids, mask)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
            logits_flat = logits.reshape(-1, logits.size(-1))
            labels_flat = labels.reshape(-1)
            loss = criterion(logits_flat, labels_flat)
            test_loss += loss.item()
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            preds = torch.argmax(logits, dim=-1)
            all_preds.extend(preds.cpu().flatten().tolist())
            all_trues.extend(labels.cpu().flatten().tolist())
    
    accuracy = accuracy_score(all_trues, all_preds)
    avg_loss = test_loss / len(loader)
    
    print(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
    print(f"   Test Loss: {avg_loss:.4f}")
    print(f"   Test Accuracy: {accuracy:.4f}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(all_trues)}")
    
    return accuracy, avg_loss
#------------------------------------------------------------------------------------
def analyze_predictions(model, loader, tokenizer, num_examples=5):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã"""
    model.eval()
    
    bad_cases = []  # –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    good_cases = []  # –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    
    with torch.no_grad():
        for batch in loader:
            ids = batch['data'].to(device)
            mask = batch['mask'].to(device)
            labels = batch['target'].to(device)
            
            logits = model(ids, mask)
            preds = torch.argmax(logits, dim=-1)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –≤ –±–∞—Ç—á–µ
            for i in range(ids.size(0)):
                # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–Ω—É–ª–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–∞–¥–¥–∏–Ω–≥)
                non_pad_indices = mask[i].bool()
                
                if non_pad_indices.sum() == 0:
                    continue
                
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (–Ω–µ –ø–∞–¥–¥–∏–Ω–≥)
                input_ids_real = ids[i][non_pad_indices]
                preds_real = preds[i][non_pad_indices]
                labels_real = labels[i][non_pad_indices]
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–æ–∫–µ–Ω—ã
                input_tokens = tokenizer.convert_ids_to_tokens(input_ids_real.cpu())
                true_tokens = tokenizer.convert_ids_to_tokens(labels_real.cpu())
                pred_tokens = tokenizer.convert_ids_to_tokens(preds_real.cpu())
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å IndexError
                min_length = min(len(input_tokens)-1, len(true_tokens), len(pred_tokens))
                
                for j in range(min_length):
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –≤—ã—Ö–æ–¥–∏–º –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã –º–∞—Å—Å–∏–≤–æ–≤
                    if j >= len(true_tokens) or j >= len(pred_tokens):
                        continue
                        
                    context = input_tokens[:j+1]  # –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
                    true_tok = true_tokens[j]
                    pred_tok = pred_tokens[j]
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ –ø–∞–¥–¥–∏–Ω–≥
                    skip_tokens = ['[PAD]', '[CLS]', '[SEP]', '<pad>', '<cls>', '<sep>', '']
                    if true_tok in skip_tokens or pred_tok in skip_tokens:
                        continue
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –Ω–æ —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                    if true_tok == pred_tok and true_tok in skip_tokens:
                        continue
                    
                    if true_tok != pred_tok:
                        bad_cases.append((context, true_tok, pred_tok))
                    else:
                        good_cases.append((context, true_tok, pred_tok))
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–±–æ—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            if len(bad_cases) > 200 and len(good_cases) > 200:
                break
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø–æ–∫–∞–∑–∞
    if not bad_cases and not good_cases:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("   - –í—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ")
        print("   - –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π")
        print("   - –°–ª–∏—à–∫–æ–º —Å—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤")
        return [], []
    
    # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    random.seed(42)
    
    # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π sampling —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏
    bad_cases_sampled = []
    good_cases_sampled = []
    
    if bad_cases:
        bad_cases_sampled = random.sample(bad_cases, min(num_examples, len(bad_cases)))
    if good_cases:
        good_cases_sampled = random.sample(good_cases, min(num_examples, len(good_cases)))
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "="*60)
    print("üîç –ê–ù–ê–õ–ò–ó –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï")
    print("="*60)
    
    if bad_cases_sampled:
        print(f"\n‚ùå –ü—Ä–∏–º–µ—Ä—ã –ù–ï–ü–†–ê–í–ò–õ–¨–ù–´–• –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ({len(bad_cases_sampled)} –∏–∑ {len(bad_cases)}):")
        print("-" * 50)
        for i, (context, true_tok, pred_tok) in enumerate(bad_cases_sampled, 1):
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Ç–æ–∫–µ–Ω–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–∏–ª–∏ –≤—Å–µ –µ—Å–ª–∏ –º–µ–Ω—å—à–µ)
            context_to_show = context[-5:] if len(context) > 5 else context
            context_str = ' '.join(context_to_show)
            
            print(f"{i}. –ö–æ–Ω—Ç–µ–∫—Å—Ç: ...{context_str}")
            print(f"   –ò—Å—Ç–∏–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω: '{true_tok}' | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π: '{pred_tok}'")
            print(f"   –°—Ç–∞—Ç—É—Å: {'üö´ –û–®–ò–ë–ö–ê' if true_tok != pred_tok else '‚úÖ –í–ï–†–ù–û'}")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–∞—Ö
            true_len = len(true_tok)
            pred_len = len(pred_tok)
            if true_len != pred_len:
                print(f"   –†–∞–∑–Ω–∏—Ü–∞ –¥–ª–∏–Ω—ã: {true_len} vs {pred_len}")
            print()
    else:
        print(f"\n‚úÖ –ù–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –ø–æ–∫–∞–∑–∞!")
    
    if good_cases_sampled:
        print(f"\n‚úÖ –ü—Ä–∏–º–µ—Ä—ã –ü–†–ê–í–ò–õ–¨–ù–´–• –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ({len(good_cases_sampled)} –∏–∑ {len(good_cases)}):")
        print("-" * 50)
        for i, (context, true_tok, pred_tok) in enumerate(good_cases_sampled, 1):
            context_to_show = context[-5:] if len(context) > 5 else context
            context_str = ' '.join(context_to_show)
            
            print(f"{i}. –ö–æ–Ω—Ç–µ–∫—Å—Ç: ...{context_str}")
            print(f"   –ò—Å—Ç–∏–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω: '{true_tok}' | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π: '{pred_tok}'")
            print(f"   –°—Ç–∞—Ç—É—Å: {'‚úÖ –í–ï–†–ù–û' if true_tok == pred_tok else 'üö´ –û–®–ò–ë–ö–ê'}")
            print()
    else:
        print(f"\n‚ùå –ù–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –ø–æ–∫–∞–∑–∞!")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_predictions = len(bad_cases) + len(good_cases)
    if total_predictions > 0:
        accuracy = len(good_cases) / total_predictions * 100
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:")
        print(f"   –í—Å–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {total_predictions}")
        print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö: {len(good_cases)} ({accuracy:.2f}%)")
        print(f"   –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö: {len(bad_cases)} ({100-accuracy:.2f}%)")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if bad_cases:
            avg_context_length = sum(len(context) for context, _, _ in bad_cases) / len(bad_cases)
            print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö: {avg_context_length:.1f} —Ç–æ–∫–µ–Ω–æ–≤")
    else:
        print(f"\nüìä –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    
    return bad_cases, good_cases

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
def show_detailed_examples(model, test_loader, tokenizer, num_examples=3):
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏"""
    model.eval()
    
    print("\n" + "="*60)
    print("üî¨ –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ê–ë–û–¢–´ –ú–û–î–ï–õ–ò")
    print("="*60)
    
    examples_shown = 0
    with torch.no_grad():
        for batch in test_loader:
            if examples_shown >= num_examples:
                break
                
            ids = batch['data'].to(device)
            mask = batch['mask'].to(device)
            labels = batch['target'].to(device)
            
            logits = model(ids, mask)
            preds = torch.argmax(logits, dim=-1)
            
            for i in range(min(num_examples - examples_shown, ids.size(0))):
                print(f"\nüìù –ü—Ä–∏–º–µ—Ä {examples_shown + 1}:")
                print("-" * 40)
                
                # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–Ω—É–ª–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
                non_pad_indices = mask[i].bool()
                input_ids_real = ids[i][non_pad_indices]
                preds_real = preds[i][non_pad_indices]
                labels_real = labels[i][non_pad_indices]
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–∫—Å—Ç
                input_tokens = tokenizer.convert_ids_to_tokens(input_ids_real.cpu())
                input_text = tokenizer.convert_tokens_to_string(input_tokens)
                
                true_tokens = tokenizer.convert_ids_to_tokens(labels_real.cpu())
                true_text = tokenizer.convert_tokens_to_string(true_tokens)
                
                pred_tokens = tokenizer.convert_ids_to_tokens(preds_real.cpu())
                pred_text = tokenizer.convert_tokens_to_string(pred_tokens)
                
                print(f"–í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç: {input_text}")
                print(f"–û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥: {true_text}")
                print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥: {pred_text}")
                
                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
                print("\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º:")
                min_len = min(len(true_tokens), len(pred_tokens))
                for j in range(min_len):
                    status = "‚úÖ" if true_tokens[j] == pred_tokens[j] else "‚ùå"
                    print(f"  {status} –ü–æ–∑–∏—Ü–∏—è {j}: '{true_tokens[j]}' vs '{pred_tokens[j]}'")
                
                examples_shown += 1
                print()
#-----------------------------------------------------------------------------------------
# –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
train_losses = []
val_accuracies = []
val_losses = []
best_val_acc = 0
best_epoch = 0
patience = 5
patience_counter = 0
# –æ–±—É—á–µ–Ω–∏–µ
print("üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π)...")
epoch_loop = tqdm(range(10), desc="–û–±—É—á–µ–Ω–∏–µ", unit="—ç–ø–æ—Ö–∞")
for epoch in epoch_loop:
    # –û–±—É—á–µ–Ω–∏–µ
    train_loss  = train_epoch(model, train_loader)    
    train_losses.append(train_loss)
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    val_acc, val_loss = evaluate(model, val_loader)
    val_accuracies.append(val_acc)
    val_losses.append(val_loss)
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_epoch = epoch + 1
        patience_counter = 0        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        best_model_path = model_dir / 'best_model.pth'
        save_model(model, optimizer, epoch, val_acc, val_loss, best_model_path)
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å accuracy: {val_acc:.4f}")
    else:
        patience_counter += 1
    # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ tqdm: –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ loss –∏ accuracy
    epoch_loop.set_postfix({
        "Loss": f"{train_loss:.4f}",
        "Val Acc": f"{val_acc:.4f}",
        "Best Acc": f"{best_val_acc:.4f}",
        "Patience": f"{patience_counter}/{patience}"
    })
    # print(f"Epoch {epoch+1}: Loss = {loss:.4f}, Accuracy = {acc:.4f}")
    # if (epoch + 1) % 10 == 0:
    #     print(f"\n–≠–ø–æ—Ö–∞ {epoch+1}: Loss = {loss:.4f}, Accuracy = {acc:.4f}")
    # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
    if patience_counter >= patience:
        print(f"üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch + 1}")
        break

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
final_model_path = model_dir / 'final_model.pth'
save_model(model, optimizer, epoch, val_acc, train_loss, final_model_path)
print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å")

import matplotlib.pyplot as plt

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –∏–∑ —ç–ø–æ—Ö–∏ {best_epoch}...")
checkpoint = torch.load(model_dir / 'best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])

# –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
print("\nüß™ –ù–∞—á–∏–Ω–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...")
test_accuracy, test_loss = test_model(model, test_loader)



# # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
# results = {
#     'best_validation_accuracy': best_val_acc,
#     'best_epoch': best_epoch,
#     'test_accuracy': test_accuracy,
#     'test_loss': test_loss,
#     'final_training_epochs': epoch + 1
# }
results = {
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    'best_validation_accuracy': best_val_acc,
    'best_epoch': best_epoch,
    'test_accuracy': test_accuracy,
    'test_loss': test_loss,
    'final_training_epochs': epoch + 1,
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—ã–±–æ—Ä–∫–∞–º
    'dataset_statistics': {
        'number_of_input_samples':limit,
        'train_samples': len(train_ds),
        'validation_samples': len(val_ds),
        'test_samples': len(test_ds),
        'total_samples': len(train_ds) + len(val_ds) + len(test_ds),
        'train_ratio': len(train_ds) / (len(train_ds) + len(val_ds) + len(test_ds)),
        'validation_ratio': len(val_ds) / (len(train_ds) + len(val_ds) + len(test_ds)),
        'test_ratio': len(test_ds) / (len(train_ds) + len(val_ds) + len(test_ds))
    },
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    'model_architecture': {
        'model_type': 'NextPhrasePredictionRNN',
        'rnn_type': rnn_type,  # –∏–ª–∏ 'LSTM', 'GRU' –µ—Å–ª–∏ –∏–∑–º–µ–Ω–∏—Ç–µ
        'vocab_size': tokenizer.vocab_size,
        'embedding_dim': v_emb_dim,
        'hidden_dim': v_hidden_dim,
        'max_sequence_length': MAX_LEN,
        'dropout_rate': 0.5
    },
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    'training_parameters': {
        'batch_size': v_batch_size,
        'learning_rate': 3e-3,
        'optimizer': 'AdamW',
        'loss_function': 'CrossEntropyLoss',
        'early_stopping_patience': patience,
        'gradient_clip_norm': 1.0
    },
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    'additional_statistics': {
        'total_training_steps': len(train_loader) * (epoch + 1),
        'final_train_loss': train_losses[-1] if train_losses else None,
        'final_validation_loss': val_losses[-1] if val_losses else None,
        'training_time_epochs': epoch + 1
    }
}

timestamp = datetime.now().strftime('%Y_%m_%d_%H_%M')
results_filename = f'training_results_{timestamp}.json'
with open(model_dir / results_filename, 'w') as f:
    json.dump(results, f, indent=2)
print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_filename}")

print(f"\nüìä –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
print(f"   –õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è accuracy: {best_val_acc:.4f}")
print(f"   –õ—É—á—à–∞—è —ç–ø–æ—Ö–∞: {best_epoch}")
print(f"   –¢–µ—Å—Ç–æ–≤–∞—è accuracy: {test_accuracy:.4f}")
print(f"   –¢–µ—Å—Ç–æ–≤–∞—è loss: {test_loss:.4f}")

#--------------------------------------------------------------------------------------------
# –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:
print("\n" + "="*60)
print("–ù–∞—á–∏–Ω–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
print("="*60)

# –ê–Ω–∞–ª–∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
bad_cases, good_cases = analyze_predictions(model, test_loader, tokenizer, num_examples=5)

# –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
show_detailed_examples(model, test_loader, tokenizer, num_examples=3)

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –æ—à–∏–±–æ–∫
def analyze_error_patterns(bad_cases, tokenizer):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫"""
    print("\n" + "="*60)
    print("üìà –ê–ù–ê–õ–ò–ó –ü–ê–¢–¢–ï–†–ù–û–í –û–®–ò–ë–û–ö")
    print("="*60)
    
    if not bad_cases:
        print("–ù–µ—Ç –æ—à–∏–±–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞! üéâ")
        return
    
    # –ê–Ω–∞–ª–∏–∑ —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –æ—à–∏–±–æ–∫
    error_pairs = [(true, pred) for _, true, pred in bad_cases]
    error_counter = Counter(error_pairs)
    
    print("\nüîù –¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –æ—à–∏–±–æ–∫:")
    for (true_tok, pred_tok), count in error_counter.most_common(10):
        print(f"  '{true_tok}' ‚Üí '{pred_tok}': {count} —Ä–∞–∑")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –¥–ª–∏–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤
    length_errors = []
    for _, true_tok, pred_tok in bad_cases:
        length_diff = abs(len(true_tok) - len(pred_tok))
        length_errors.append(length_diff)
    
    if length_errors:
        avg_length_diff = sum(length_errors) / len(length_errors)
        print(f"\nüìè –°—Ä–µ–¥–Ω—è—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –¥–ª–∏–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö: {avg_length_diff:.2f}")

# –í—ã–∑—ã–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—à–∏–±–æ–∫
analyze_error_patterns(bad_cases, tokenizer)

#--------------------------------------------------------------------------------------------

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
plt.figure(figsize=(15, 5))

# –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
plt.subplot(1, 3, 1)
plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', color='tab:blue', linewidth=2,marker=None)
plt.plot(range(1, len(val_losses) + 1), val_losses, label='Val Loss', color='tab:red', linewidth=2,marker=None)
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()

# –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
plt.subplot(1, 3, 2)
plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy', color='tab:orange', linewidth=2,marker=None)
plt.axhline(y=best_val_acc, color='red', linestyle='--', label=f'Best Acc: {best_val_acc:.4f}')
plt.title('Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.grid(True)
plt.legend()

# –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
plt.subplot(1, 3, 3)
metrics = ['Best Val Acc', 'Test Acc']
values = [best_val_acc, test_accuracy]
colors = ['lightblue', 'lightgreen']
bars = plt.bar(metrics, values, color=colors)
plt.title('Final Metrics Comparison')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
for bar, value in zip(bars, values):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
             f'{value:.4f}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig('training_results.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
print(f"üìÅ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {model_dir}")
print(f"üìä –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: training_results.png")