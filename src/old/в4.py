from tqdm import tqdm  # ‚Üê –î–û–ë–ê–í–õ–ï–ù–û
import pandas as pd
import re 
from collections import Counter
from pathlib import Path
import html
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizerFast
import torch
from torch.utils.data import Dataset, DataLoader
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
from transformers import AutoTokenizer
file_path = Path(__file__).parent.parent / 'data' / '1_raw_dataset_tweets.txt'
print(file_path)
model_dir="C:/Users/OMEN/Documents/LLM_Test/YaPracticum/project1_text-autocomplete/models"
# v_path="../data/raw_dataset_tweets.txt"
# v_path="C:/Users/OMEN/Documents/LLM_Test/YaPracticum/project1_text-autocomplete/data/raw_dataset_tweets.txt"
# texts_df = pd.read_csv(file_path)
# print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(dtexts_df)}")

# –ü—Ä–æ—Å—Ç–æ —á–∏—Ç–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏
with open(file_path, 'r', encoding='utf-8') as f:
    texts = f.readlines()

# –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç—Ä–æ–∫
texts = [text.strip() for text in texts]
texts = texts[:100]

print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Å—Ç—Ä–æ–∫")
# for i, text in enumerate(texts[:5]):  # –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫
#     print(f"{i+1}: {text}")
    
def clean_text(text):
    text = text.lower()  # –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É    
    text = html.unescape(text)  # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç &quot; -> ", &amp; -> &, –∏ —Ç.–¥.# 2. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å HTML —Å–∏–º–≤–æ–ª—ã  
    text = re.sub(r'<[^>]+>', '', text)  # 3. –£–¥–∞–ª–∏—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è HTML —Ç–µ–≥–∏
    text = re.sub(r'@\w+', 'user', text)# –∑–∞–º–µ–Ω–∏—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è (@username)    
    text = re.sub(r'#\w+', '', text) # –£–¥–∞–ª–∏—Ç—å —Ö–µ—à—Ç–µ–≥–∏ (#tag)    
    text = re.sub(r'[^\w\s,\.!?;:]', '', text) # –£–¥–∞–ª–∏—Ç—å —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)# –£–¥–∞–ª–∏—Ç—å —Å—Å—ã–ª–∫–∏    
   
    text = re.sub(r"[^a-z0-9 ]+", " ", text)  # –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã
    text = re.sub(r"\s+", " ", text).strip()  # —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã
    
    return text
print("Cleanning...")
cleaned_texts =[]
for text in tqdm(texts):
    cleaned_texts.append(clean_text(text))
    
text_pairs = [(i, texts[i], cleaned_texts[i]) for i in range(len(texts))]

#—Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ csv
texts_df = pd.DataFrame(text_pairs, columns=['rowno','text_raw','text_cleaned'])
# output_path = Path(__file__).parent.parent / 'data' / '2_cleaned_dataset_tweets.csv'
# texts_df.to_csv(output_path, index=False, encoding='utf-8')

print("Cleanning READY!")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

MAX_LEN = 20  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–∏–º–µ—Ä–∞ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
print("Prepearing X, Y pairs...")
data = []
for text in tqdm(texts_df['text_cleaned']):  # df['clean'] ‚Äî –æ—á–∏—â–µ–Ω–Ω—ã–µ —Ç–≤–∏—Ç—ã
    tokens = tokenizer.tokenize(text)
    if len(tokens) < 2:# –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
        continue
    # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–≤–∏—Ç–æ–≤ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞ —Ä–∞–∑–º–µ—Ä–æ–º MAX_LEN
    for i in range(1, len(tokens)):
        start_idx = max(0, i - MAX_LEN)
        x_tok = tokens[start_idx:i]# —Ç–æ–∫–µ–Ω—ã —Å start_idx –¥–æ i-1        
        y_tok = tokens[start_idx+1:i+1]# –¶–µ–ª—å: —Ç–µ –∂–µ —Ç–æ–∫–µ–Ω—ã, –Ω–æ —Å–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞ 1 –≤–ø–µ—Ä–µ–¥ (—Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω)
        if len(x_tok) < 1 or len(y_tok) < 1:
            continue
        # padding —Å–ª–µ–≤–∞ –¥–ª—è X/Y –¥–æ MAX_LEN
        # x_ids = [0] * (MAX_LEN-len(x_tok)) + [vocab.get(tok, 1) for tok in x_tok]
        # y_ids = [0] * (MAX_LEN-len(y_tok)) + [vocab.get(tok, 1) for tok in y_tok]
        # data.append((x_ids, y_ids))
        # data.append((x_tok,y_tok))
         # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–∫–µ–Ω—ã –≤ ID
        x_ids = tokenizer.convert_tokens_to_ids(x_tok)
        y_ids = tokenizer.convert_tokens_to_ids(y_tok)                
        # –ü–∞–¥–¥–∏–Ω–≥ —Å–ª–µ–≤–∞ –¥–ª—è X/Y –¥–æ max_len
        x_padded = [tokenizer.pad_token_id] * (MAX_LEN - len(x_ids)) + x_ids
        y_padded = [tokenizer.pad_token_id] * (MAX_LEN - len(y_ids)) + y_ids                
        # –°–æ–∑–¥–∞–µ–º attention_mask (1 –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, 0 –¥–ª—è –ø–∞–¥–¥–∏–Ω–≥–∞)
        attention_mask = [1 if token_id != tokenizer.pad_token_id else 0 for token_id in x_padded]
        # print(i,x_tok,y_tok,x_padded, y_padded,attention_mask)
        # print()
        data.append((x_padded, y_padded,attention_mask))
print("PAIRS ARE READY!")


class TweetsDataset(Dataset):
    def __init__(self, data, targets,mask):
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ data –∏ targets –≤ –∞—Ç—Ä–∏–±—É—Ç—ã
        # –í–∞—à –∫–æ–¥ –∑–¥–µ—Å—å
        self.data = data
        self.targets = targets 
        self.mask=mask

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):

        return {'data':torch.tensor(self.data[idx], dtype=torch.long),
                'target':torch.tensor(self.targets[idx], dtype=torch.long),
                'mask':torch.tensor(self.mask[idx], dtype=torch.long)                
            }

from sklearn.model_selection import train_test_split 
train, temp = train_test_split(data, test_size=0.2, random_state=42)
val, test = train_test_split(temp, test_size=1/2, random_state=42)  # val 0.1, test 0.2

#—Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–æ—Ä—Ç–µ–∂–µ–π —á–µ—Ä–µ–∑ zip
X_train, Y_train, M_train = zip(*train)
X_val, Y_val, M_val = zip(*val)
X_test, Y_test, M_test = zip(*test)

train_ds = TweetsDataset(X_train, Y_train, M_train)
val_ds = TweetsDataset(X_val, Y_val, M_val)
test_ds = TweetsDataset(X_test, Y_test, M_test)

train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=128)
test_loader = DataLoader(test_ds, batch_size=128)

print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç—ã —Å–æ–∑–¥–∞–Ω—ã: Train={len(train_ds)}, Val={len(val_ds)}, Test={len(test_ds)}")

# for batch_idx, batch in enumerate(train_loader):
#     if batch_idx >= 1:
#         break
#     print("BATCH Number=",batch_idx)
#     for batch_item_num, batch_item in enumerate(batch):
#         print(batch_item_num)
#         print("x=",batch['data'][batch_item_num])
#         print("y=",batch['target'][batch_item_num])
#         print("m=",batch['mask'][batch_item_num])
 
# –ü—Ä–æ–≤–µ—Ä–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
# print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π:")
# sample_batch = next(iter(train_loader))
# print(f"inputs shape: {sample_batch['data'].shape}")  # [128, 20]
# print(f"labels shape: {sample_batch['target'].shape}")        # [128, 20] - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å!
# print(f"attention_mask shape: {sample_batch['mask'].shape}")  # [128, 20]


import torch.nn as nn
from sklearn.metrics import accuracy_score

class MeanPoolingRNN(nn.Module):
    def __init__(self, vocab_size, emb_dim=300, hidden_dim=256, output_dim=2, pad_idx=0):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)
        self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)# RNN-—Å–ª–æ–π
        self.norm = nn.LayerNorm(hidden_dim) # —Å–ª–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.dropout = nn.Dropout(0.5)# dropout-—Å–ª–æ–π
        self.fc = nn.Linear(hidden_dim, vocab_size) # –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self.init_weights()


    def init_weights(self):
        # –Ω–∞–ø–∏—à–∏—Ç–µ xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤
        # if isinstance(self, nn.Linear) or isinstance(self, nn.RNN):
        #     nn.init.xavier_uniform_(self.weight)
        #     if self.bias is not None:
        #         nn.init.zeros_(self.bias)
        # nn.init.xavier_uniform_(self.fc.weight)
        # for name, param in self.rnn.named_parameters():
        #     if 'weight' in name:
        #         nn.init.xavier_uniform_(param)
        for name, param in self.named_parameters():
            if 'weight' in name and param.dim() > 1:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0.0)


    def forward(self, input_ids, attention_mask):
        x = self.embedding(input_ids)
        rnn_out, _ = self.rnn(x)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç rnn-—Å–ª–æ—è
        
        rnn_out_normed = self.norm(rnn_out)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è rnn_out


        # mean pooling –ø–æ attention_mask
        # mask = attention_mask.unsqueeze(2).expand_as(rnn_out)# –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –º–∞—Å–∫—É –¥–ª—è rnn_out_normed
        mask = attention_mask.unsqueeze(-1).float()  # [batch_size, seq_len, 1]
        # mask = mask.unsqueeze(-1).float()
        masked_out =  rnn_out_normed * mask# —É–º–Ω–æ–∂—å—Ç–µ rnn_out_normed –Ω–∞ –º–∞—Å–∫—É
        summed = masked_out.sum(dim=1)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Å—É–º–º—É masked_out –ø–æ dim=1
        # lengths = attention_mask.sum(dim=1).unsqueeze(1)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        lengths = attention_mask.sum(dim=1).unsqueeze(1).clamp(min=1e-9)  # [batch_size, 1]
        mean_pooled = summed / lengths# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Å—Ä–µ–¥–Ω–∏–µ, —Ä–∞–∑–¥–µ–ª–∏–≤ —Å—É–º–º—ã –Ω–∞ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
         # 5. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        # –ü–æ–≤—Ç–æ—Ä—è–µ–º –æ–±—â–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        seq_representation = mean_pooled.unsqueeze(1).repeat(1, input_ids.size(1), 1)  # [batch_size, seq_len, hidden_dim]
        
        # out = self.dropout(mean_pooled)# –ø—Ä–∏–º–µ–Ω–∏—Ç–µ dropout
        out = self.dropout(seq_representation)
        
        logits = self.fc(out)# –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π 
        return logits


# —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
pad_token_id = tokenizer.pad_token_id  # –¥–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ pad_token_id
# model = MeanPoolingRNN(vocab_size=tokenizer.vocab_size, pad_idx=pad_token_id).to(device)
model = MeanPoolingRNN(
    vocab_size=tokenizer.vocab_size,
    emb_dim=256,
    hidden_dim=512,
    pad_idx=pad_token_id
).to(device)

# —Å–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)#, weight_decay=3e-3) # —Å–æ–∑–¥–∞–π—Ç–µ –æ–ø—Ç–∏–º–∞–π–∑–µ—Ä —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)


# –∫–æ–¥ –æ–±—É—á–µ–Ω–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏
def train_epoch(model, loader):
    model.train()
    total_loss = 0
    progress_bar = tqdm(loader, desc="Train", leave=False)
    for batch in progress_bar:
        ids = batch['data'].to(device)
        mask = batch['mask'].to(device)
        labels = batch['target'].to(device)



        optimizer.zero_grad()# –æ–±–Ω—É–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        logits = model(ids, mask)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏
        
        # print(f"Logits shape: {logits.shape}")  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å [128, 2]
        # print(f"Labels shape: {labels.shape}")  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å [128]
        # print(f"Labels dtype: {labels.dtype}")  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å torch.int64
        logits_flat = logits.reshape(-1, logits.size(-1))
        labels_flat = labels.reshape(-1)
        
        # loss = criterion(logits, labels)# –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å
        loss = criterion(logits_flat, labels_flat)
        loss.backward()  # –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # –ø—Ä–∏–º–µ–Ω–∏—Ç–µ gradient clipping
        optimizer.step() # –æ–±–Ω–æ–≤–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

        total_loss += loss.item()
        # –û–±–Ω–æ–≤–ª—è–µ–º postfix tqdm: –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π loss
        progress_bar.set_postfix({"Loss": f"{loss.item():.4f}"})

        
        
    return total_loss / len(loader)


# –∫–æ–¥ –ø–æ–¥—Å—á—ë—Ç–∞ accuracy –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
def evaluate(model, loader):
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for batch in loader:
            ids = batch['data'].to(device)
            mask = batch['mask'].to(device)
            labels = batch['target']
            logits = model(ids, mask)
            # preds += torch.argmax(logits, dim=1).cpu().tolist()
            # trues += labels.tolist()
            preds += torch.argmax(logits, dim=-1).cpu().flatten().tolist()
            trues += labels.flatten().tolist()
    return accuracy_score(trues, preds)

def save_model(model, optimizer, epoch, accuracy, loss, path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ –µ—ë –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'accuracy': accuracy,
        'loss': loss,
        'model_config': {
            'vocab_size': tokenizer.vocab_size,
            'emb_dim': 256,
            'hidden_dim': 512,
            'pad_idx': pad_token_id
        }
    }, path)

def load_model(model, optimizer, path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –µ—ë –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"""
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    return checkpoint['epoch'], checkpoint['accuracy'], checkpoint['loss']

def test_model(model, loader):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    model.eval()
    all_preds = []
    all_trues = []
    test_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Testing"):
            ids = batch['data'].to(device)
            mask = batch['mask'].to(device)
            labels = batch['target'].to(device)
            
            logits = model(ids, mask)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
            logits_flat = logits.reshape(-1, logits.size(-1))
            labels_flat = labels.reshape(-1)
            loss = criterion(logits_flat, labels_flat)
            test_loss += loss.item()
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            preds = torch.argmax(logits, dim=-1)
            all_preds.extend(preds.cpu().flatten().tolist())
            all_trues.extend(labels.cpu().flatten().tolist())
    
    accuracy = accuracy_score(all_trues, all_preds)
    avg_loss = test_loss / len(loader)
    
    print(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
    print(f"   Test Loss: {avg_loss:.4f}")
    print(f"   Test Accuracy: {accuracy:.4f}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(all_trues)}")
    
    return accuracy, avg_loss

# –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
train_losses = []
val_accuracies = []

# –æ–±—É—á–µ–Ω–∏–µ
print("üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π)...")
epoch_loop = tqdm(range(10), desc="–û–±—É—á–µ–Ω–∏–µ", unit="—ç–ø–æ—Ö–∞")
for epoch in epoch_loop:
    loss = train_epoch(model, train_loader)
    acc = evaluate(model, val_loader)
    train_losses.append(loss)
    val_accuracies.append(acc)
    # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ tqdm: –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ loss –∏ accuracy
    epoch_loop.set_postfix({
        "Loss": f"{loss:.4f}",
        "Acc": f"{acc:.4f}"
    })
    # print(f"Epoch {epoch+1}: Loss = {loss:.4f}, Accuracy = {acc:.4f}")
    # if (epoch + 1) % 10 == 0:
    #     print(f"\n–≠–ø–æ—Ö–∞ {epoch+1}: Loss = {loss:.4f}, Accuracy = {acc:.4f}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
final_model_path = model_dir / 'final_model.pth'
save_model(model, optimizer, epoch, val_acc, train_loss, final_model_path)
print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å")

import matplotlib.pyplot as plt

# –°–æ–∑–¥–∞—ë–º –≥—Ä–∞—Ñ–∏–∫
plt.figure(figsize=(12, 5))

# –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å (loss)
plt.subplot(1, 2, 1)
plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', color='tab:blue')#marker='o'
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()

# –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ (accuracy)
plt.subplot(1, 2, 2)
plt.plot(range(1, len(val_accuracies) + 1), val_accuracies,  label='Validation Accuracy', color='tab:orange')#marker='s',
plt.title('Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.savefig('training_curves.png', dpi=150)
plt.show()