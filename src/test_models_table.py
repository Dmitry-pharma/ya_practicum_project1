import torch
from torch.utils.data import DataLoader
from pathlib import Path
from transformers import AutoTokenizer, GPT2LMHeadModel
import random
import pandas as pd
from data_utils import load_and_clean_data, prepare_training_pairs
from next_token_dataset import TweetsDataset
from lstm_model import NextPhrasePredictionRNN
from eval_lstm import vevaluate
from sklearn.model_selection import train_test_split
import evaluate

def create_test_dataset(file_path, limit=1000, max_len=20):
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É –Ω–∞ 1000 –∑–∞–ø–∏—Å–µ–π"""
    print("üìä –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    texts_df = load_and_clean_data(file_path, limit)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º BertTokenizer –¥–ª—è LSTM –º–æ–¥–µ–ª–∏
    lstm_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM
    data = prepare_training_pairs(texts_df, lstm_tokenizer, max_len)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–Ω–µ —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test)
    test_data = data
    
    X_test, Y_test, M_test = zip(*test_data)
    
    # –°–æ–∑–¥–∞–µ–º dataset –∏ loader –¥–ª—è LSTM
    test_ds = TweetsDataset(X_test, Y_test, M_test)
    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)
    
    print(f"‚úÖ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∞: {len(test_ds)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    return test_loader, lstm_tokenizer, texts_df

def load_lstm_model(model_path, device):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç LSTM –º–æ–¥–µ–ª—å"""
    checkpoint = torch.load(model_path, map_location=device)
    model_config = checkpoint['model_config']
    
    model = NextPhrasePredictionRNN(
        rnn_type="LSTM",
        vocab_size=model_config['vocab_size'],
        emb_dim=model_config['emb_dim'],
        hidden_dim=model_config['hidden_dim'],
        pad_idx=model_config['pad_idx']
    ).to(device)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    return model, model_config

def generate_lstm_completion(model, tokenizer, text, device, max_length=20, num_tokens=5):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é LSTM –º–æ–¥–µ–ª–∏"""
    model.eval()
    
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=max_length)
    input_ids = inputs['input_ids'].to(device)
    
    generated_tokens = []
    
    with torch.no_grad():
        for _ in range(num_tokens):
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
            outputs = model(input_ids, torch.ones_like(input_ids))
            next_token_logits = outputs[:, -1, :]
            next_token = torch.argmax(next_token_logits, dim=-1)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –∫ –≤—Ö–æ–¥—É
            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)
            generated_tokens.append(next_token.item())
            
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
            if next_token.item() == tokenizer.sep_token_id or len(generated_tokens) >= num_tokens:
                break
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    return generated_text

def generate_gpt2_completion(model, tokenizer, text, device, max_length=50, num_tokens=10):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é GPT2 –º–æ–¥–µ–ª–∏"""
    model.eval()
    
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=max_length)
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    
    with torch.no_grad():
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ
        outputs = model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=input_ids.shape[1] + num_tokens,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,
            temperature=0.1,
            no_repeat_ngram_size=2
        )
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
    generated_ids = outputs[0][input_ids.shape[1]:]
    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    return generated_text

def calculate_rouge(prediction, reference):
    """–í—ã—á–∏—Å–ª—è–µ—Ç ROUGE –º–µ—Ç—Ä–∏–∫–∏ –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –∏ —ç—Ç–∞–ª–æ–Ω–æ–º"""
    try:
        rouge = evaluate.load('rouge')
        results = rouge.compute(
            predictions=[prediction],
            references=[reference],
            use_stemmer=True
        )
        return {
            'rouge1': round(results['rouge1'], 4),
            'rouge2': round(results['rouge2'], 4),
            'rougeL': round(results['rougeL'], 4)
        }
    except Exception as e:
        return {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}

def get_actual_continuation(full_text, context_length=0.7):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
    words = full_text.split()
    if len(words) <= 5:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
        return ""
    
    split_point = max(5, int(len(words) * context_length))
    context = " ".join(words[:split_point])
    continuation = " ".join(words[split_point:])
    
    return context, continuation

def create_comparison_table(texts_df, lstm_model, lstm_tokenizer, gpt2_model, gpt2_tokenizer, device, num_examples=15):
    """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    print("\n" + "="*60)
    print("üìä –°–û–ó–î–ê–ù–ò–ï –¢–ê–ë–õ–ò–¶–´ –°–†–ê–í–ù–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô")
    print("="*60)
    
    # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    random.seed(42)
    sample_texts = random.sample(texts_df['text_raw'].tolist(), min(num_examples, len(texts_df)))
    
    results = []
    
    for i, full_text in enumerate(sample_texts):
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–º–µ—Ä–∞ {i+1}/{len(sample_texts)}...")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ
            context, actual_continuation = get_actual_continuation(full_text)
            
            if not context or not actual_continuation:
                continue
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏
            lstm_continuation = generate_lstm_completion(
                lstm_model, lstm_tokenizer, context, device, num_tokens=10
            )
            
            gpt2_continuation = generate_gpt2_completion(
                gpt2_model, gpt2_tokenizer, context, device, num_tokens=15
            )
            
            # –í—ã—á–∏—Å–ª—è–µ–º ROUGE –º–µ—Ç—Ä–∏–∫–∏
            lstm_rouge = calculate_rouge(lstm_continuation, actual_continuation)
            gpt2_rouge = calculate_rouge(gpt2_continuation, actual_continuation)
            
            results.append({
                '–ò—Å—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ': full_text[:100] + "..." if len(full_text) > 100 else full_text,
                '–ù–∞—á–∞–ª–æ —Å–æ–æ–±—â–µ–Ω–∏—è': context[:80] + "..." if len(context) > 80 else context,
                '–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ': actual_continuation[:80] + "..." if len(actual_continuation) > 80 else actual_continuation,
                'LSTM –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ': lstm_continuation[:80] + "..." if len(lstm_continuation) > 80 else lstm_continuation,
                'DistilGPT2 –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ': gpt2_continuation[:80] + "..." if len(gpt2_continuation) > 80 else gpt2_continuation,
                'ROUGE-L LSTM': lstm_rouge['rougeL'],
                'ROUGE-L DistilGPT2': gpt2_rouge['rougeL'],
                'ROUGE-1 LSTM': lstm_rouge['rouge1'],
                'ROUGE-1 DistilGPT2': gpt2_rouge['rouge1']
            })
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–∏–º–µ—Ä–∞ {i+1}: {e}")
            continue
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame(results)
    
    # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    column_order = [
        '–ò—Å—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ',
        '–ù–∞—á–∞–ª–æ —Å–æ–æ–±—â–µ–Ω–∏—è', 
        '–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ',
        'LSTM –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ',
        'DistilGPT2 –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ',
        'ROUGE-L LSTM',
        'ROUGE-L DistilGPT2',
        'ROUGE-1 LSTM', 
        'ROUGE-1 DistilGPT2'
    ]
    
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã
    existing_columns = [col for col in column_order if col in df.columns]
    df = df[existing_columns]
    
    return df

def main():
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º –∏ –º–æ–¥–µ–ª–∏
    data_path = Path(__file__).parent.parent / 'data' / '1_raw_dataset_tweets.txt'
    model_path = Path(__file__).parent.parent / 'models' / 'best_model.pth'
    
    # 1) –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É –Ω–∞ 1000 –∑–∞–ø–∏—Å–µ–π
    test_loader, lstm_tokenizer, texts_df = create_test_dataset(
        file_path=data_path, 
        limit=1000,
        max_len=20
    )
    
    # 2) –ó–∞–≥—Ä—É–∂–∞–µ–º LSTM –º–æ–¥–µ–ª—å
    lstm_model = None
    if model_path.exists():
        try:
            print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ LSTM –º–æ–¥–µ–ª–∏...")
            lstm_model, model_config = load_lstm_model(model_path, device)
            lstm_model.eval()
            print("‚úÖ LSTM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ LSTM –º–æ–¥–µ–ª–∏: {e}")
            return
    else:
        print(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ LSTM –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
        return
    
    # 3) –ó–∞–≥—Ä—É–∂–∞–µ–º DistilGPT2 –º–æ–¥–µ–ª—å
    try:
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ DistilGPT2 –º–æ–¥–µ–ª–∏...")
        gpt2_tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
        gpt2_model = GPT2LMHeadModel.from_pretrained("distilgpt2").to(device)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º pad token –¥–ª—è GPT2
        if gpt2_tokenizer.pad_token is None:
            gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token
            
        gpt2_model.eval()
        print("‚úÖ DistilGPT2 –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ DistilGPT2 –º–æ–¥–µ–ª–∏: {e}")
        return
    
    # 4) –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    comparison_df = create_comparison_table(
        texts_df, 
        lstm_model, 
        lstm_tokenizer, 
        gpt2_model, 
        gpt2_tokenizer, 
        device,
        num_examples=150
    )
    
    # 5) –í—ã–≤–æ–¥–∏–º —Ç–∞–±–ª–∏—Ü—É
    print("\n" + "="*80)
    print("üìã –¢–ê–ë–õ–ò–¶–ê –°–†–ê–í–ù–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô (15 —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤)")
    print("="*80)
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ pandas
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', 50)
    
    # –í—ã–≤–æ–¥–∏–º —Ç–∞–±–ª–∏—Ü—É
    print(comparison_df.to_string(index=False))
    
    # 6) –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ —Ñ–∞–π–ª
    output_path = Path(__file__).parent / "model_comparison_results.csv"
    comparison_df.to_csv(output_path, index=False, encoding='utf-8')
    print(f"\nüíæ –¢–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_path}")
    
    # 7) –í—ã–≤–æ–¥–∏–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    print("\n" + "="*60)
    print("üìà –°–†–ï–î–ù–ò–ï –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê")
    print("="*60)
    
    if 'ROUGE-L LSTM' in comparison_df.columns and 'ROUGE-L DistilGPT2' in comparison_df.columns:
        avg_rouge_l_lstm = comparison_df['ROUGE-L LSTM'].mean()
        avg_rouge_l_gpt2 = comparison_df['ROUGE-L DistilGPT2'].mean()
        avg_rouge1_lstm = comparison_df['ROUGE-1 LSTM'].mean()
        avg_rouge1_gpt2 = comparison_df['ROUGE-1 DistilGPT2'].mean()
        
        print(f"üìä LSTM –º–æ–¥–µ–ª—å:")
        print(f"   –°—Ä–µ–¥–Ω–∏–π ROUGE-L: {avg_rouge_l_lstm:.4f}")
        print(f"   –°—Ä–µ–¥–Ω–∏–π ROUGE-1: {avg_rouge1_lstm:.4f}")
        
        print(f"\nüìä DistilGPT2 –º–æ–¥–µ–ª—å:")
        print(f"   –°—Ä–µ–¥–Ω–∏–π ROUGE-L: {avg_rouge_l_gpt2:.4f}")
        print(f"   –°—Ä–µ–¥–Ω–∏–π ROUGE-1: {avg_rouge1_gpt2:.4f}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if avg_rouge_l_lstm > avg_rouge_l_gpt2:
            winner = "LSTM"
            diff = avg_rouge_l_lstm - avg_rouge_l_gpt2
        else:
            winner = "DistilGPT2"
            diff = avg_rouge_l_gpt2 - avg_rouge_l_lstm
        
        print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ ROUGE-L: {winner}")
        print(f"   –†–∞–∑–Ω–∏—Ü–∞: {diff:.4f}")
    
    print(f"\n‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == "__main__":
    main()